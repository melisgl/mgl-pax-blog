<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Recurrent Nets</title>
<link type='text/css' href='style.css' rel='stylesheet'/>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
   </script>
   </head>
<body>
<div id="content-container">
<div id="toc">
<div class="menu-block"><span class="menu-block-title">me</span><ul><li><a href="http://quotenil.com">blog</a></li><li><a href="mailto:mega@retes.hu">email</a></li><li><a href="http://github.com/melisgl/">git</a></li></ul></div><div id="page-toc">
</div>
<div id="toc-footer"><ul><li><a href="https://github.com/melisgl/mgl-pax">[generated by MGL-PAX]</a></li></ul></div>
</div>
<div id="content">
<p><a id='x-28MGL-PAX-BLOG-3A-40RECURRENT-NETS-20MGL-PAX-3ASECTION-29'></a></p>

<p><span class="outer-navigation"><span class="navigation"> <a href="#x-28MGL-PAX-BLOG-3A-40RECURRENT-NETS-20MGL-PAX-3ASECTION-29" title="Recurrent Nets">&#8634;</a></span></span></p>

<h1><a href="#x-28MGL-PAX-BLOG-3A-40RECURRENT-NETS-20MGL-PAX-3ASECTION-29">Recurrent Nets</a></h1>

<p><em>Tags</em>: <a href="category-ai.html#x-28MGL-PAX-BLOG-3A-3A-40CATEGORY-AI-20MGL-PAX-3ASECTION-29" title="AI">AI</a>, <a href="category-lisp.html#x-28MGL-PAX-BLOG-3A-3A-40CATEGORY-LISP-20MGL-PAX-3ASECTION-29" title="Lisp">Lisp</a>, <a href="blog.html#x-28MGL-PAX-BLOG-3A-3A-40BLOG-20MGL-PAX-3ASECTION-29" title="Blog">Blog</a></p>

<p><em>2015-01-19</em> -- I've been cleaning up and documenting
<a href="https://github.com/melisgl/mgl" >MGL</a> for quite some time now and
while it's nowhere near done, a good portion of the code has been
overhauled in the process. There are new additions such as the <a href="http://arxiv.org/abs/1412.6980" >Adam
optimizer</a> and Recurrent Neural
Nets. My efforts were mainly only the backprop stuff and I think the
definition of feed-forward:</p>

<pre><code><span class="code"><span class="paren1">(<span class="code">build-fnn <span class="paren2">(<span class="code"><span class="keyword">:class</span> 'digit-fnn</span>)</span>
  <span class="paren2">(<span class="code">input <span class="paren3">(<span class="code">-&gt;input <span class="keyword">:size</span> <span class="special">*n-inputs*</span></span>)</span></span>)</span>
  <span class="paren2">(<span class="code">hidden-activation <span class="paren3">(<span class="code">-&gt;activation input <span class="keyword">:size</span> n-hiddens</span>)</span></span>)</span>
  <span class="paren2">(<span class="code">hidden <span class="paren3">(<span class="code">-&gt;relu hidden-activation</span>)</span></span>)</span>
  <span class="paren2">(<span class="code">output-activation <span class="paren3">(<span class="code">-&gt;activation hidden <span class="keyword">:size</span> <span class="special">*n-outputs*</span></span>)</span></span>)</span>
  <span class="paren2">(<span class="code">output <span class="paren3">(<span class="code">-&gt;softmax-xe-loss <span class="keyword">:x</span> output-activation</span>)</span></span>)</span></span>)</span></span></code></pre>

<p>and recurrent nets:</p>

<pre><code><span class="code"><span class="paren1">(<span class="code">build-rnn <span class="paren2">(<span class="code"></span>)</span>
  <span class="paren2">(<span class="code">build-fnn <span class="paren3">(<span class="code"><span class="keyword">:class</span> 'sum-sign-fnn</span>)</span>
    <span class="paren3">(<span class="code">input <span class="paren4">(<span class="code">-&gt;input <span class="keyword">:size</span> 1</span>)</span></span>)</span>
    <span class="paren3">(<span class="code">h <span class="paren4">(<span class="code">-&gt;lstm input <span class="keyword">:size</span> n-hiddens</span>)</span></span>)</span>
    <span class="paren3">(<span class="code">prediction <span class="paren4">(<span class="code">-&gt;softmax-xe-loss
                 <span class="paren5">(<span class="code">-&gt;activation h <span class="keyword">:name</span> 'prediction <span class="keyword">:size</span> <span class="special">*n-outputs*</span></span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>

<p>is fairly straight-forward already. There is still much code that
needs to accompany such a network definition, mostly having to do
with how to give inputs and prediction targets to the network and
also with monitoring training. See the full examples for
<a href="https://github.com/melisgl/mgl/blob/master/doc/md/mgl-manual.md#x-28MGL-BP-3A-40MGL-FNN-TUTORIAL-20MGL-PAX-3ASECTION-29" >feed-forward</a>
and
<a href="https://github.com/melisgl/mgl/blob/master/doc/md/mgl-manual.md#x-28MGL-BP-3A-40MGL-RNN-TUTORIAL-20MGL-PAX-3ASECTION-29" >recurrent</a>
nets in the documentation.</p>
  </div>
</div>
<script>$('#page-toc').toc({'selectors': 'h1,h2,h3,h4'});</script>
</body>
</html>
