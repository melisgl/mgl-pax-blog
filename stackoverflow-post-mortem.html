<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Stackoverflow Post-Mortem</title>
<link type='text/css' href='style.css' rel='stylesheet'>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width">
   <script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
   </script>
   <link href='https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,500;0,700;1,500;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,500;0,700;1,500;1,700&display=swap'); </style>
<link href='https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap'); </style>

</head>
<body>
<div id="content-container">
<div id=toc>
 <div id=home>
  <a href=blog.html>(QUOTE NIL)</a>
 </div>
 <div id=links>
  Ramblings on <a href=ai.html>ai</a>, <a href=lisp.html>lisp</a>, <a
  href=tech.html>tech</a> and <a href=personal.html>personal</a> topics by <a
  href=about-me.html>me</a>.
 </div>
</div><div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40STACKOVERFLOW-POST-MORTEM-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@STACKOVERFLOW-POST-MORTEM%20MGL-PAX:SECTION"></a></p>

<h1><a href="stackoverflow-post-mortem.html">Stackoverflow Post-Mortem</a></h1>

<p><em>Tags</em>: <a href="ai.html" title="`ai`"><code>ai</code></a>, <a href="lisp.html" title="`lisp`"><code>lisp</code></a>, <em>Date</em>: <code>2013-04-09</code></p>

<p>After almost two years without a single
competition, last September I decided to enter the
<a href="http://www.kaggle.com/c/predict-closed-questions-on-stack-overflow" >Stackoverflow</a>
contest on <a href="http://kaggle.com" >Kaggle</a>. It was a straightforward
text classification problem with extremely unbalanced classes.</p>

<p>Just as Bocsimack√≥ did the last time around, his lazier sidekick</p>

<p><img src="blog-files/malacka-es-bocsimacko.jpg" alt="Malacka" /></p>

<p>(on the right) brought
<a href="http://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/leaderboard" >success</a>.
I would have loved to be lazy and still win, but the leaderboard was
too close for comfort.</p>

<h2>Overview</h2>

<p>The winning model is an average of 10 neural network ensembles of
five constituent models, three of which are Deep Belief Networks,
one is logistic regression, and one is Vowpal Wabbit. Features are
all binary and include handcrafted, binned indicators (time of post,
length of title, etc) and unigrams from the title and body.</p>

<p>Since the data set - especially the class distribution - evolves
with time, one crucial step is to compensate for the effect of time.
This is partly accomplished by adding date and time information as
features, and also by training the ensemble on the most recent
posts.</p>

<p>Since the constituent models are trained on a subset of the
stratified sample provided by the organizer, the ensemble does two
of things:</p>

<ul>
<li><p>Blend the constituent models, duh.</p></li>
<li><p>Compensate for the differences between the stratified sample and
  the most recent months of the full training set.</p></li>
</ul>

<h2>Features Selection / Extraction</h2>

<p>Didn't spend too much time on handcrafting the features, just played
around with adding features one-by-one, keeping an eye on how the loss
changes. These are all binary features. For example (:post-hour 8) is
8 o'clock UTC, (:post-hour 11) is 11 o'clock UTC.</p>

<p>Depending on the model the top-N features are used, where the features
are sorted by log likelihood ratio. There were a number of other
feature selection methods tried, see below.</p>

<h2>Modeling Techniques and Training</h2>

<p>First let's look one by one at the models that went into the ensemble.</p>

<h3>Deep Belief Networks</h3>

<p>A DBN is made of boltzmann machines stacked on top of each other,
trained in a layerwise manner. After training (called 'pretraining')
the DBN is 'unrolled' into a backpropagation network that's trained
(called 'fine-tuning') to minimize the cross entropy between the
predicted and actual class probabilities.</p>

<p>There are three DBNs in the ensemble. The first one looks like this
(omitted the biases for clarity):</p>

<pre><code>LABEL(5) INPUTS(2000)
       /
    F1(400)
      |
    F2(800)
</code></pre>

<p>So we have 5 softmax neurons in the LABEL chunk, representing the
class probabilities. There are 2000 sigmoid neuron in the INPUTS
chunk standing for the top 2000 binary features extracted from the
post. Then we have two hidden layers of sigmoid neurons: F1 and F2.
This is created in the code by <code>MAKE-MALACKA-DBN-SMALL</code>.</p>

<p>The second DBN is the same expect INPUTS, F1 and F2 have 10000, 800,
800 neurons respectively. See <code>MAKE-MALACKA-DBN-BIG</code>.</p>

<p>The third DBN is the same expect INPUTS, F1 and F2 have 10000, 2000,
2000 neurons respectively. See <code>MAKE-MALACKA-DBN-BIGGER</code>. Note that
this last DBN wasn't fine tuned due to time constraints; predictions
are extracted directly from the DBN that doesn't try to minimize
cross entropy.</p>

<p>The RBMs in the DBN were trained with contrastive divergence with
mini batches of 100 posts. Learning rate was 0.001, momentum 0.9,
weight decay 0.0002.</p>

<p>The backprop networks were trained for 38 epochs with the conjugate
gradient method with three line searches on batches of 10000 posts.
For the first 5 epochs, only the softmax units were trained, and for
the last 3 epochs there was only one batch epoch (i.e. normal
conjugate gradient).</p>

<p>These guys take several hours to days to train.</p>

<h3>Logistic Regression</h3>

<p>Not much to say here. I used liblinear with the top 250000 features,
with these parameters:</p>

<pre><code>:solver-type :l2r-lr
:c 256
:eps 0.001
</code></pre>

<p>Even though it had access to a much larger set of features,
liblinear could only achieve ~0.83 on the stratified sample used for
development vs ~0.79 for the second DBN. Still, even though they
used the same kind of features, they were different enough to
slightly improve in the ensemble.</p>

<h3>Vowpal Wabbit</h3>

<p>I'm not sure adding this helped at all in the end, the results
weren't entirely convincing. I just took Foxtrot's code. VW is run
with <code>--loss_function logistic --oaa 5</code>.</p>

<h3>The ensemble</h3>

<p>The ensemble is a backpropagation neural network with one hidden
layer of 800 stochastic sigmoid neurons (at least that was the
intention, see below). The network looked like this:</p>

<pre><code>PRED1 PRED2 PRED3 PRED4 PRED5
     _______|___/____/
            OUTPUT(800)
              |
         CROSS-ENTROPY
</code></pre>

<p>PRED1 is made of five neurons representing the class probabilities
in the prediction of the first DBN. The rest of PRED* are for the
other two DBNs, the liblinear model, and VW.</p>

<p>The network was trained with gradient descent with mini batches of
100 posts. Learning rate started out as 0.01 and multiplies by 0.98
each epoch. Momentum started out as 0.5 and was increased to 0.99 in
50 epochs. Learning rate was also multiplied by (1 - momentum) to
disentangle it from the momentum. No weight decay was used.</p>

<p>I tried to get Hinton's dropout technique working, but it didn't
live up to my expectations. On the other hand, stochastic binary
neurons mentioned in the dropout presentation, did help a tiny bit.
Unfortunately, I managed to make the final submission with a broken
version where the weights of stochastic binary neurons were not
trained at all, effectively resulting in 800 random features (!).</p>

<h3>Bagging</h3>

<p>As good as stochastic binary neurons were before I broke the code,
it still helped a tiny bit (as in a couple of 0.0001s) to average 10
ensembles.</p>

<h2>Additional Comments and Observations</h2>

<h3>Time</h3>

<p>It was clear from the beginning that time plays an important role,
and if scores are close then predicting the class distribution of
the test set could be the deciding factor. I saw the pace of
change (with regards to distribution of classes) picking up near the
end of the development training set and probed into the public
leaderboard by submitting a number different constant
predictions (the same prior for every post). It seemed that the last
two weeks or one month is best.</p>

<p>There was no obvious seasonality or trend that could be exploited on
the scale of months. I checked whether stackoverflow were changing
the mechanics, but didn't find anything. I certainly didn't foresee
the drastic class distribution change that was to come.</p>

<h3>Features</h3>

<p>I tried a couple of feature extraction methods. The
Key-Substring-Group extractor looked very promising, but it simply
didn't scale to more than a thousand features.</p>

<p>In the end, I found that no important features were left out by
playing with liblinear that could handle all features at the same
time. Take it with a grain of salt, of course, because there is
noise/signal issue lurking.</p>

<h3>Naive Bayes, random forests, gradient boosting</h3>

<p>I experimented with the above in scikit-learn. The results were
terrible, but worse, they didn't contribute to the ensemble either.
Maybe it was only me.</p>

<h3>Libsvm</h3>

<p>I couldn't get it to scale to several tens of thousands posts so I
had to go with liblinear.</p>

<h3>Dropout</h3>

<p>Fine tuning DBNs with dropout or stochastic binary neurons (without
the bugs) didn't work. The best I could achive was slightly worse
than the conjugate gradient based score.</p>

<h3>Retraining consituent models</h3>

<p>Recall that the consituent models were trained only on 4/5 of the
available data. After the ensemble was trainined, I intended to
replace retrain them on the whole stratified training set. Initial
experiments with liblinear were promising, but with the DBN the
public leaderboard score got a lot worse and I ran out of time to
experiment.</p>
  </div>
</div>
<script>$('#page-toc').toc({'selectors': ''});</script>
</body>
</html>
