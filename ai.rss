<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>Category AI in G&#xE1;bor Melis&apos; Blog</title>
        <link>http://quotenil.com/ai.html</link>
        <description>Category AI in G&#xE1;bor Melis&apos; Blog</description>
        <generator>xml-emitter</generator>
        <language>en-uk</language>
        <item>
            <title>Practitioner&apos;s Guide to Two-Tailed Averaging</title>
            <link>http://quotenil.com/tta-practitioner.html</link>
            <description>&lt;p&gt;This is a complement to the &lt;a href=&quot;https://arxiv.org/abs/2209.12581&quot; &gt;Two-Tailed Averaging
paper&lt;/a&gt;, approached from the
direction of what I think is a fairly common technique: averaging
checkpoints.&lt;/p&gt;

&lt;p&gt;We want to speed up training and improve generalization. One way to
do that is by averaging weights from optimization, and that&apos;s a big
win (e.g. &lt;a href=&quot;https://arxiv.org/abs/1708.02182&quot; &gt;1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot; &gt;2&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2209.14981&quot; &gt;3&lt;/a&gt;). For example, while
training a language model for the down-stream task of summarization,
we can save checkpoints periodically and average the model weights
from the last 10 or so checkpoints to produce the final solution.
This is pretty much what &lt;a href=&quot;https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/&quot; &gt;Stochastic Weight
Averaging&lt;/a&gt; (SWA) does.&lt;/p&gt;

&lt;h2&gt;Problems with SWA&lt;/h2&gt;

&lt;p&gt;There is a number of problems with SWA:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The averaging length (e.g. 10) must be chosen to maximize
  performance on summarization.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A naive way to find the averaging length is to do a single
  training run and then search backwards extending the average one
  checkpoint at a time, which needs lots of storage and computation.
  Another option, doing multiple training runs each told to start
  averaging at a predefined point pays a steep price in computation
  for lower storage costs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To control the costs, we can lower checkpointing fequency, but
  does that make results worse? We can test that with multiple
  training runs and pay the cost there.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, how do we know when to stop training? We ideally want to
  stop training the language model when summarization works best
  with the optimal averaging length at that point. That means the
  naive search has to be run periodically making it even more
  expensive.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, working with SWA is tricky because:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The averaging length is a hyperparameter that&apos;s costly to set (it
  is coupled to other hyperparameters especially to the length of
  training and the learning rate).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Determining the averaging length after training is both costly (in
  storage and/or computation) and suboptimal (can miss early
  solutions).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are the issues Two-Tailed Averaging tackles. &lt;/p&gt;

&lt;h2&gt;Two-Tailed Averaging&lt;/h2&gt;

&lt;p&gt;The algorithm needs storage for only two sets of weights (constant
storage cost) and performance (e.g. of summarization) to be
evaluated periodically. In return, it provides a weight average of
approximately optimal length at all optimization steps. Now, we can
start training that language model, periodically evaluating how the
averaged weights are doing at summarization. We can stop the
training run any time if it&apos;s getting worse.&lt;/p&gt;

&lt;p&gt;This is how Two-Tailed Averaged (orange) compares to SWA (green)
tuned to start averaging at the point that&apos;s optimal for final
validation loss:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/tta-vs-swa.png&quot; alt=&quot;2TA (orange) vs SWA (green)&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;The Algorithm&lt;/h2&gt;

&lt;p&gt;The core algorithm maintains two weight averages. Both averages are
over the most recent weights weight produced by the optimizer, but
they differ in length (i.e. how many weights they average). As the
optimizer produces new sets of weights, they are added to both
averages. We periodically evaluate the performance of our model with
each average. If the short average (the one that currently has fewer
weights averaged) does at least as well as the long average
according to some arbitrary evaluation function, then we empty the
long average, which will now be the short one.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Initialize the short (s, sw) and long averages (l, lw). s and l are
&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;# the number of weights averaged (the &quot;averaging lengths&quot;). sw and lw
&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;# are the averaged weights.
&lt;/span&gt;s, sw, l, lw = 0, 0, 0, 0

&lt;span class=&quot;comment&quot;&gt;# Update the averages with the latest weights from the optimizer.
&lt;/span&gt;&lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; update_2ta&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;:
  &lt;span class=&quot;symbol&quot;&gt;global&lt;/span&gt; s, sw, l, lw
  &lt;span class=&quot;symbol&quot;&gt;assert&lt;/span&gt; s &amp;lt;= l
  s, sw = s+1, &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;s*sw + w&lt;/span&gt;)&lt;/span&gt;/&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;s+1&lt;/span&gt;)&lt;/span&gt;
  l, lw = l+1, &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;l*lw + w&lt;/span&gt;)&lt;/span&gt;/&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;l+1&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;comment&quot;&gt;# Evaluate the model with the short-, the long-, and the
&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;# non-averaged weights. Based on the results, adapt the length of
&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;# the averages. Return three values: the best evaluation results,
&lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;# the corresponding weights and averaging length.
&lt;/span&gt;&lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; evaluate_2ta&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w, evaluate&lt;/span&gt;)&lt;/span&gt;:
  &lt;span class=&quot;symbol&quot;&gt;global&lt;/span&gt; s, sw, l, lw
  &lt;span class=&quot;comment&quot;&gt;# Evaluate the non-averaged weights w, the short and the long average.
&lt;/span&gt;  f1, fs, fl = evaluate&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;, evaluate&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;sw&lt;/span&gt;)&lt;/span&gt;, evaluate&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;lw&lt;/span&gt;)&lt;/span&gt;
  is_first_eval = &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;s == l&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;comment&quot;&gt;# If the short average is better, then *switch*: empty the long
&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# average, which is now the shorter one.
&lt;/span&gt;  &lt;span class=&quot;symbol&quot;&gt;if&lt;/span&gt; fs &amp;lt;= fl:
    s, l, lw, fl = 0, s, sw, fs
  &lt;span class=&quot;symbol&quot;&gt;if&lt;/span&gt; f1 &amp;lt;= fl:
    &lt;span class=&quot;comment&quot;&gt;# The non-averaged weights performed better. This may happen in
&lt;/span&gt;    &lt;span class=&quot;comment&quot;&gt;# the very early stages of training.
&lt;/span&gt;    &lt;span class=&quot;symbol&quot;&gt;if&lt;/span&gt; is_first_eval:
      &lt;span class=&quot;comment&quot;&gt;# If there has never been a switch (s == l), then f1 is probably
&lt;/span&gt;      &lt;span class=&quot;comment&quot;&gt;# still improving fast so reset both averages.
&lt;/span&gt;      s, l = 0, 0
    &lt;span class=&quot;symbol&quot;&gt;return&lt;/span&gt; f1, w, 1
  else:
    &lt;span class=&quot;comment&quot;&gt;# Return the long average.
&lt;/span&gt;    &lt;span class=&quot;symbol&quot;&gt;return&lt;/span&gt; fl, lw, l&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to the core algorithm, the code above has some extra
logic to deal with the non-averaged weights being better than the
averaged ones.&lt;/p&gt;

&lt;p&gt;Let&apos;s write a fake a training loop that optimizes $f(x)=x^2$.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;symbol&quot;&gt;import&lt;/span&gt; random

&lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; test_2ta_simple&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;:
  &lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; f&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;:
    &lt;span class=&quot;symbol&quot;&gt;return&lt;/span&gt; w**2
  &lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; df_dw&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;:
    &lt;span class=&quot;comment&quot;&gt;# Simulate stochasticity due to e.g. minibatching.
&lt;/span&gt;    &lt;span class=&quot;symbol&quot;&gt;return&lt;/span&gt; 2*w + random.uniform&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-1.0, 1.0&lt;/span&gt;)&lt;/span&gt;
  lr = 0.5
  w = 3.14
  &lt;span class=&quot;symbol&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;symbol&quot;&gt;in&lt;/span&gt; range&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;1, 2001&lt;/span&gt;)&lt;/span&gt;:
    w = w - lr*df_dw&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;
    update_2ta&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;symbol&quot;&gt;if&lt;/span&gt; i % 100 == 0:
      f_2ta, w_2ta, l_2ta = evaluate_2ta&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w, f&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;symbol&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;f&lt;span class=&quot;string&quot;&gt;&apos;i={i:4d}: f(w_i)={f(w):7.3f},&apos;&lt;/span&gt;
            f&lt;span class=&quot;string&quot;&gt;&apos; f(w_2ta)={f_2ta:7.3f}, l={l_2ta:4d}&apos;&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We added some noise to the gradients in &lt;code&gt;df_dw&lt;/code&gt; to make it more like
training a neural net with SGD. Anyway, we take 2000 optimization
steps, calling &lt;code&gt;update_2ta&lt;/code&gt; on most but calling
&lt;code&gt;update_and_evaluate_2ta&lt;/code&gt; every 100 steps. Running
&lt;code&gt;test_2ta_simple&lt;/code&gt;, we get something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;i= 100: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.108, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 100
i= 200: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.011, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 200
i= 300: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.098, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 200
i= 400: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.085, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 300
i= 500: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.221, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 200
i= 600: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.185, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 300
i= 700: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.019, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 400
i= 800: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.180, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 500
i= 900: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.161, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 600
i=1000: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.183, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 700
i=1100: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.057, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 800
i=1200: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.045, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 900
i=1300: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.051, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l=1000
i=1400: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.010, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l= 900
i=1500: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.012, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l=1000
i=1600: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.168, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l=1100
i=1700: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.001, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l=1200
i=1800: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.020, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l=1300
i=1900: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.090, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l=1400
i=2000: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=0.115, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=0.000, l=1500&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above, &lt;code&gt;f(w_i)&lt;/code&gt; is the loss with the non-averaged weights,
&lt;code&gt;f(w_2ta)&lt;/code&gt; is the loss with the weights provided by 2TA, and &lt;code&gt;l&lt;/code&gt; is
the number of weights averaged. We see that with the high, constant
learning rate, SGD keeps jumping around the optimum, and while 2TA
does the same, its jitter is way smaller (it&apos;s beyond the three
significant digits printed here). Also, the length of the average
increases almost monotonically but not quite due to the switching
logic.&lt;/p&gt;

&lt;p&gt;OK, that was easy. Let&apos;s now do something a bit more involved, where
the function being optimized changes. We will change the loss
function to $f(x) = (x-m)^2$ where $m$ is set randomly every 400
steps. We will deal with this non-stationarity by resetting the long
average if it has not improved for a while.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; reset_2ta_long_average&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;:
  &lt;span class=&quot;symbol&quot;&gt;global&lt;/span&gt; s, sw, l, lw
  s, sw, l, lw = 0, 0, s, sw

&lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; test_2ta_non_stationary&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;:
  optimum = 0
  &lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; f&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;:
    &lt;span class=&quot;symbol&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w-optimum&lt;/span&gt;)&lt;/span&gt;**2
  &lt;span class=&quot;special&quot;&gt;def&lt;/span&gt;&lt;span
class=&quot;keyword&quot;&gt; df_dw&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;:
    &lt;span class=&quot;comment&quot;&gt;# Simulate stochasticity due to e.g. minibatching.
&lt;/span&gt;    &lt;span class=&quot;symbol&quot;&gt;return&lt;/span&gt; 2*w - 2*optimum + random.uniform&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-1.0, 1.0&lt;/span&gt;)&lt;/span&gt;
  lr = 0.5
  w = 3.14
  best_f = float&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;inf&quot;&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  best_iteration = 0
  &lt;span class=&quot;symbol&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;symbol&quot;&gt;in&lt;/span&gt; range&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;1, 2001&lt;/span&gt;)&lt;/span&gt;:
    w = w - lr*df_dw&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;
    update_2ta&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;symbol&quot;&gt;if&lt;/span&gt; i % 400 == 0:
      optimum = random.uniform&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-10.0, 10.0&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;symbol&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;f&lt;span class=&quot;string&quot;&gt;&apos;setting optimum={optimum:.3f}&apos;&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;symbol&quot;&gt;if&lt;/span&gt; i % 100 == 0:
      f_2ta, w_2ta, l_2ta = evaluate_2ta&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w, f&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;symbol&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;f&lt;span class=&quot;string&quot;&gt;&apos;i={i:4d}: f(w_i)={f(w):7.3f},&apos;&lt;/span&gt;
            f&lt;span class=&quot;string&quot;&gt;&apos; f(w_2ta)={f_2ta:7.3f}, l={l_2ta:4d}&apos;&lt;/span&gt;,
            end=&lt;span class=&quot;string&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;symbol&quot;&gt;if&lt;/span&gt; l_2ta &amp;gt; 1 &lt;span class=&quot;symbol&quot;&gt;and&lt;/span&gt; f_2ta &amp;lt; best_f:
        best_f = f_2ta
        best_iteration = i
        &lt;span class=&quot;symbol&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;symbol&quot;&gt;elif&lt;/span&gt; best_iteration + 1 &amp;lt;= i:
        &lt;span class=&quot;comment&quot;&gt;# Reset heuristic: the results of the long average have not
&lt;/span&gt;        &lt;span class=&quot;comment&quot;&gt;# improved for a while, let&apos;s reset it so that it may adapt
&lt;/span&gt;        &lt;span class=&quot;comment&quot;&gt;# quicker.
&lt;/span&gt;        &lt;span class=&quot;symbol&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;string&quot;&gt;&apos; Reset!&apos;&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
        reset_2ta_long_average&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;
        best_f = float&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;string&quot;&gt;&quot;inf&quot;&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
        best_iteration = 0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that 2TA adapts to the non-stationarity in a reasonable
way although the reset heuristic gets triggered spuriously a couple
of times:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;i= 100: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.008, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.005, l= 100
i= 200: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.060, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 100
i= 300: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.004, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 100
setting optimum=9.691
i= 400: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;= 87.194, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;= 87.194, l=   1 Reset!
i= 500: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.002, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 100
i= 600: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.033, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 200 Reset!
i= 700: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.126, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 200
setting optimum=9.899
i= 800: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.022, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.022, l=   1 Reset!
i= 900: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.004, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.003, l= 100
i=1000: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.094, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 100
i=1100: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.146, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 100
setting optimum=3.601
i=1200: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;= 35.623, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;= 35.623, l=   1 Reset!
i=1300: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.113, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.001, l= 100
i=1400: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.166, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 200
i=1500: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.112, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 200
setting optimum=6.662
i=1600: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;= 11.692, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  9.409, l= 300 Reset!
i=1700: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.075, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 100
i=1800: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.229, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 200 Reset!
i=1900: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=  0.217, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=  0.000, l= 100
setting optimum=-8.930
i=2000: f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_i&lt;/span&gt;)&lt;/span&gt;=242.481, f&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;w_2ta&lt;/span&gt;)&lt;/span&gt;=242.481, l=   1 Reset!&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that in these examples the evaluation function in 2TA was the
training loss, but 2TA is intended for when the evaluation function
measures performance on the validation set or on a down-stream
task (e.g. summarization).&lt;/p&gt;

&lt;h2&gt;Scaling to Large Models&lt;/h2&gt;

&lt;p&gt;In its proposed form, Two-Tailed Averaging incorporates every set of
weights produced by the optimizer in both averages it maintains.
This is good because &lt;a href=&quot;https://jmlr.org/papers/v18/16-595.html&quot; &gt;Tail Averaging&lt;/a&gt;, also known as
&lt;a href=&quot;https://arxiv.org/abs/1109.5647&quot; &gt;Suffix Averaging&lt;/a&gt;, theory has nice things to say
about convergence to a local optimum of the training losss in this
setting. However, in a memory constrained situation, these averages
will not fit on the GPU/TPU, so we must move the weights off the
device to add them to the averages (which may be in RAM or on disk).
Moving stuff off the device can be slow, so we might want to do
that, say, every 20 optimization steps. Obviously, downsampling the
weights too much will affect the convergence rate, so there is a
tradeoff.&lt;/p&gt;

&lt;h2&gt;Learning Rate&lt;/h2&gt;

&lt;p&gt;Note that in our experiments with Two-Tailed Averaging, we used a
constant learning rate motivated by the fact that the closely
related method of Tail Averaging guarantees optimal convergence rate
learning rate in such a setting. The algorithm should work with
decreasing learning rates but would require modification for
cyclical schedules.&lt;/p&gt;

&lt;h2&gt;Related Works&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot; &gt;SWA&lt;/a&gt; averages the last $K$ checkpoints.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.14981&quot; &gt;LAWA&lt;/a&gt; averages the $K$ most recent checkpoints, so it
  produces reasonable averages from early on (unlike SWA), but $K$
  still needs to be set manually.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.02182&quot; &gt;NT-ASGD&lt;/a&gt; starts averaging when the validation loss has
  not improved for a fixed number of optimization steps, which
  trades one hyperparameter for another, and it is sensitive to
  noise in the raw validation loss.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Adaptivity&lt;/strong&gt;: SWA and LAWA have hyperparameters that directly
control the averaging length; NT-ASGD still has one, but its effect
is more indirect. &lt;strong&gt;Anytimeness&lt;/strong&gt;: LAWA provides an average at all
times, SWA and NT-ASGD don&apos;t. &lt;strong&gt;Optimality&lt;/strong&gt;: The final averages of
SWA and LAWA are optimal if their hyperparameters are well-tuned;
intermediate results of LAWA are unlikely to be optimal; NT-ASGD can
miss the right time to start averaging.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Two-Tailed Averaging can be thought of as online SWA with no
hyperparameters. It is a great option when training runs take a long
(or even an a priori unknown amount of) time, and when we could do
without optimizing yet another hyperparameter.
&lt;div class=&apos;br&apos;&gt;&lt;/div&gt;&lt;p&gt;Comment on
&lt;a href=&quot;https://twitter.com/GaborMelis/status/1600479387937144833&quot; &gt;Twitter&lt;/a&gt;
or &lt;a href=&quot;https://mastodon.social/@melisgl/109472579530491223&quot; &gt;Mastodon&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>On the Design of Matrix Libraries</title>
            <link>http://quotenil.com/on-the-design-of-matrix-libraries.html</link>
            <description>&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: &lt;em&gt;2020-05-03&lt;/em&gt; – Things have changed the during last 5
years. This is a non-issue in Tensorflow and possibly in other
frameworks, as well.&lt;/p&gt;

&lt;p&gt;I believe there is one design decision in
&lt;a href=&quot;http://melisgl.github.io/mgl-pax-world/mat-manual.html&quot; &gt;MGL-MAT&lt;/a&gt;
that has far reaching consequences: to make a single matrix object
capable of storing multiple representations of the same data and let
operations decide which representation to use based on what&apos;s the
most convenient or efficient, without having to even know about all
the possible representations.
This allows existing code to keep functioning if support for
diagonal matrices (represented as a 1d array) lands, and one can pick
and choose the operations performance critical enough to implement
with diagonals.&lt;/p&gt;

&lt;p&gt;Adding support for matrices that, for instance, live on a remote
machine is thus possible with a new facet type (MAT lingo for
representation) and existing code would continue to work (albeit
possibly slowly). Then, one could optimize the bottleneck operations
by sending commands over the network instead of copying data.&lt;/p&gt;

&lt;p&gt;Contrast this with what I understand to be the status quo over on
the Python side. The specialized Python array libs (cudamat,
gpuarray, cudandarray) try to be drop-in replacements for – or at
least similar to – numpy.ndarray with various degrees of success.
There are lots of explicit conversion going on between ndarray and
these CUDA blobs and adding new representations would make this
exponentionally worse.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://torch.ch/&quot; &gt;Torch&lt;/a&gt; (Lua) also has CUDA and non-CUDA tensors
are separate types, and copying between main and GPU memory is
explicit, which leads to pretty much the same problems.&lt;/p&gt;

&lt;p&gt;All of this is kind of understandable. When one thinks in terms of
single dispatch (i.e. &lt;code&gt;object.method()&lt;/code&gt;), this kind of design will
often emerge. With &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_dispatch&quot; &gt;muliple
dispatch&lt;/a&gt;, data
representation and operations are more loosely coupled. The
facet/operation duality of MGL-MAT is reminiscent of how CLOS
classes and generic functions relate to each other. The anology is
best if objects are allowed to shapeshift to fit the method
signatures.&lt;/p&gt;

&lt;p&gt;Speaking of multiple dispatch, by making the operations generic
functions following some kind of protocol to decide which facets and
implementation to use would decouple facets further. Ultimately,
this could make the entire CUDA related part of MGL-MAT an add-on.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Thu, 26 Feb 2015 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Recurrent Nets</title>
            <link>http://quotenil.com/recurrent-nets.html</link>
            <description>&lt;p&gt;I&apos;ve been cleaning up and documenting
&lt;a href=&quot;https://github.com/melisgl/mgl&quot; &gt;MGL&lt;/a&gt; for quite some time now, and
while it&apos;s nowhere near done, a good portion of the code has been
overhauled in the process. There are new additions such as the &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot; &gt;Adam
optimizer&lt;/a&gt; and Recurrent Neural
Nets. My efforts were mainly only the backprop stuff and I think the
definition of feed-forward:
&lt;code&gt;
(build-fnn (:class &apos;digit-fnn)
  (input (-&amp;gt;input :size *n-inputs*))
  (hidden-activation (-&amp;gt;activation input :size n-hiddens))
  (hidden (-&amp;gt;relu hidden-activation))
  (output-activation (-&amp;gt;activation hidden :size *n-outputs*))
  (output (-&amp;gt;softmax-xe-loss :x output-activation)))
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and recurrent nets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;build-rnn &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;build-fnn &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;:class&lt;/span&gt; &apos;sum-sign-fnn&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;input &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-&amp;gt;input &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; 1&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;h &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-&amp;gt;lstm input &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; n-hiddens&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;prediction &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-&amp;gt;softmax-xe-loss
                 &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-&amp;gt;activation h &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;prediction &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; &lt;span class=&quot;special&quot;&gt;*n-outputs*&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is fairly straight-forward already. There is still much code that
needs to accompany such a network definition, mostly having to do
with how to give inputs and prediction targets to the network and
also with monitoring training. See the full examples for
&lt;a href=&quot;https://github.com/melisgl/mgl/blob/master/doc/md/mgl-manual.md#x-28MGL-BP-3A-40MGL-FNN-TUTORIAL-20MGL-PAX-3ASECTION-29&quot; &gt;feed-forward&lt;/a&gt;
and
&lt;a href=&quot;https://github.com/melisgl/mgl/blob/master/doc/md/mgl-manual.md#x-28MGL-BP-3A-40MGL-RNN-TUTORIAL-20MGL-PAX-3ASECTION-29&quot; &gt;recurrent&lt;/a&gt;
nets in the documentation.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Mon, 19 Jan 2015 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Higgs Boson Challenge Bits and Pieces</title>
            <link>http://quotenil.com/higgs-boson-machine-learning-challenge-bits-and-pieces.html</link>
            <description>&lt;p&gt;The &lt;a href=&quot;http://www.kaggle.com/c/higgs-boson&quot; &gt;Higgs Boson
contest&lt;/a&gt; on
&lt;a href=&quot;http://kaggle.com&quot; &gt;Kaggle&lt;/a&gt; has ended. Sticking to my word at &lt;a href=&quot;http://medias.ircam.fr/xff38ba&quot; &gt;ELS
2014&lt;/a&gt;, I released some code that
came about during these long four months.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melisgl/mgl-gpr&quot; &gt;MGL-GPR&lt;/a&gt; is no longer a
&lt;a href=&quot;http://en.wikipedia.org/wiki/Genetic_programming&quot; &gt;Genetic
Programming&lt;/a&gt; only
library because it got another &lt;a href=&quot;http://en.wikipedia.org/wiki/Evolutionary_algorithm&quot; &gt;Evolutionary
Algorithm&lt;/a&gt;
implementation: &lt;a href=&quot;http://en.wikipedia.org/wiki/Differential_evolution&quot; &gt;Differential
Evolution&lt;/a&gt;. My
original plan for this contest was to breed input features that the
physicists in their insistence on comprehensibility overlooked, but
it didn&apos;t work as well as I had hoped for reasons specific to this
contest and also because evolutionary algorithms just do not scale
to larger problem sizes.&lt;/p&gt;

&lt;p&gt;In other news, &lt;a href=&quot;http://github.com/melisgl/mgl&quot; &gt;MGL&lt;/a&gt; got &lt;a href=&quot;http://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot; &gt;cross
validation&lt;/a&gt;,
&lt;a href=&quot;http://en.wikipedia.org/wiki/Bootstrap_aggregating&quot; &gt;bagging&lt;/a&gt; and
stratification support in the brand new
&lt;a href=&quot;https://github.com/melisgl/mgl/blob/4b1800a6dcbdf290a66b5f952fe3bd81641b0b5c/src/resample.lisp&quot; &gt;MGL-RESAMPLE&lt;/a&gt;
package documented with
&lt;a href=&quot;https://github.com/melisgl/mgl-pax&quot; &gt;MGL-PAX&lt;/a&gt;, which you all will
most definitely want to use. My winning submission used bagged
cross-validated dropout neural networks with stratified splits so
this is where it&apos;s coming from.&lt;/p&gt;

&lt;p&gt;MGL itself and &lt;a href=&quot;https://github.com/melisgl/mgl-mat&quot; &gt;MGL-MAT&lt;/a&gt; were
updated to work with the latest
&lt;a href=&quot;https://github.com/takagi/cl-cuda&quot; &gt;CL-CUDA&lt;/a&gt;. The neural network
code also saw some additions such as
&lt;a href=&quot;http://arxiv.org/abs/1312.1909&quot; &gt;-&amp;gt;MAX-CHANNEL&lt;/a&gt; activation (which
originated as &lt;a href=&quot;http://people.idsia.ch/~juergen/nips2013.pdf&quot; &gt;LWTA&lt;/a&gt;)
and also gaussian multiplicative noise. The next steps here are
further cleanups to MGL, writing documentation and moving it to
github. Also, there is some hope that one day CL-CUDA can be
included in quicklisp allowing my stuff there to be updated to their
latest versions.&lt;/p&gt;

&lt;p&gt;The code for this contest is available at
&lt;a href=&quot;https://github.com/melisgl/higgsml&quot; &gt;https://github.com/melisgl/higgsml&lt;/a&gt;,
which from now on doubles as my skeleton for lisp projects that need
to be delivered as source and as binary. It sucks in all
dependencies from quicklisp available at a certain date, clones the
necessary repositories not available in quicklisp, builds an
executable, and has a simple &lt;code&gt;make dist&lt;/code&gt; rule as well.&lt;/p&gt;

&lt;p&gt;There is also a fairly generic ensembling algorithm, which I will
factor out  later.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Tue, 23 Sep 2014 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>Higgs Boson Challenge Post-Mortem</title>
            <link>http://quotenil.com/higgs-boson-machine-learning-challenge-post-mortem.html</link>
            <description>&lt;p&gt;Actually, I&apos;ll only link to the
&lt;a href=&quot;http://www.kaggle.com/c/higgs-boson/forums/t/10344/winning-methodology-sharing/53944#post53944&quot; &gt;post-mortem&lt;/a&gt;
I wrote in the forum. There is a also a &lt;a href=&quot;https://github.com/melisgl/higgsml/blob/master/doc/model.md&quot; &gt;model
description&lt;/a&gt;
included in the &lt;a href=&quot;https://github.com/melisgl/higgsml&quot; &gt;git repo&lt;/a&gt;. A
stand-alone distribution with all library dependencies and an x86-64
linux precompiled binary is also
&lt;a href=&quot;http://quotenil.com/higgsml/gabor-melis.zip&quot; &gt;available&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This has been the Kaggle competition that attracted the most
contestants so it feels really good to come out on top although
there was an element of luck involved due to the choice of
evaluation metric and the amount of data available. The organizers
did a great job explaining the physics, why there is no more data,
motivating the choice of evaluation metric, and being prompt in
communication in general.&lt;/p&gt;

&lt;p&gt;I hope that the HEP guys will find this useful in their search for
more evidence of tau-tau decay of the Higgs boson. Note that I
didn&apos;t go for the &apos;HEP meets ML Award&apos;, so training time is
unnecessarily high (one day with a GTX Titan GPU). By switching to
single precision floating point and a single neural network,
training time could be reduced to about 15 minutes with an expected
drop in accuracy from 3.805 to about 3.750. Even with the bagging
approach, the code logs out-of-bag estimates of the evaluation
metric after training each constituent model and the training
process can be &lt;code&gt;C-c&lt;/code&gt;ed early. Furthermore, the model can be run on a
CPU with BLAS about 10 times slower than on a Titan.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Tue, 23 Sep 2014 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>Liblinear Support Added to cl-libsvm</title>
            <link>http://quotenil.com/liblinear-support-added-to-cl-libsvm.html</link>
            <description>&lt;p&gt;In addition to the cl-libsvm asdf system, there is now another asdf
system in the &lt;a href=&quot;http://github.com/melisgl/cl-libsvm&quot; &gt;cl-libsvm&lt;/a&gt;
library: cl-liblinear that, predictably enough, is a wrapper for
&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot; &gt;liblinear&lt;/a&gt;. The API
is similar to that of cl-libsvm.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Tue, 09 Apr 2013 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>Stackoverflow Post-Mortem</title>
            <link>http://quotenil.com/stackoverflow-post-mortem.html</link>
            <description>&lt;p&gt;After almost two years without a single
competition, last September I decided to enter the
&lt;a href=&quot;http://www.kaggle.com/c/predict-closed-questions-on-stack-overflow&quot; &gt;Stackoverflow&lt;/a&gt;
contest on &lt;a href=&quot;http://kaggle.com&quot; &gt;Kaggle&lt;/a&gt;. It was a straightforward
text classification problem with extremely unbalanced classes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/malacka-es-bocsimacko.jpg&quot; alt=&quot;Malacka&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Just as Bocsimack&#xF3; did the last time around, his lazier sidekick (on
the right) brought
&lt;a href=&quot;http://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/leaderboard&quot; &gt;success&lt;/a&gt;.
I would have loved to be lazy and still win, but the leaderboard was
too close for comfort.&lt;/p&gt;

&lt;h2&gt;Overview&lt;/h2&gt;

&lt;p&gt;The winning model is an average of 10 neural network ensembles of
five constituent models, three of which are Deep Belief Networks,
one is logistic regression, and one is Vowpal Wabbit. Features are
all binary and include handcrafted, binned indicators (time of post,
length of title, etc) and unigrams from the title and body.&lt;/p&gt;

&lt;p&gt;Since the data set – especially the class distribution – evolves
with time, one crucial step is to compensate for the effect of time.
This is partly accomplished by adding date and time information as
features and also by training the ensemble on the most recent posts.&lt;/p&gt;

&lt;p&gt;Since the constituent models are trained on a subset of the
stratified sample provided by the organizer, the ensemble does two
of things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Blend the constituent models, duh.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compensate for the differences between the stratified sample and
  the most recent months of the full training set.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Features Selection / Extraction&lt;/h2&gt;

&lt;p&gt;Didn&apos;t spend too much time on handcrafting the features, just played
around with adding features one-by-one, keeping an eye on how the
loss changes. These are all binary features. For
example, &lt;code&gt;(:post-hour 8)&lt;/code&gt; is 8 o&apos;clock UTC, &lt;code&gt;(:post-hour 11)&lt;/code&gt; is 11
o&apos;clock UTC.&lt;/p&gt;

&lt;p&gt;Depending on the model the top-N features are used, where the features
are sorted by log likelihood ratio. There were a number of other
feature selection methods tried, see below.&lt;/p&gt;

&lt;h2&gt;Modeling Techniques and Training&lt;/h2&gt;

&lt;p&gt;First let&apos;s look one by one at the models that went into the ensemble.&lt;/p&gt;

&lt;h3&gt;Deep Belief Networks&lt;/h3&gt;

&lt;p&gt;A DBN is made of Boltzmann machines stacked on top of each other,
trained in a layerwise manner. After training (called &apos;pretraining&apos;)
the DBN is &apos;unrolled&apos; into a backpropagation network. The BPN is
initialized with the weights of th DBN and is fine-tuned to minimize
the cross entropy between the predicted and actual class
probabilities.&lt;/p&gt;

&lt;p&gt;There are three DBNs in the ensemble. The first one looks like this
(omitted the biases for clarity):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LABEL(5) INPUTS(2000)
       /
    F1(400)
      |
    F2(800)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, we have 5 softmax neurons in the LABEL chunk, representing the
class probabilities. There are 2000 sigmoid neurons in the INPUTS
chunk standing for the top 2000 binary features extracted from the
post. Then, we have two hidden layers of sigmoid neurons: F1 and F2.
This is created in the code by &lt;code&gt;MAKE-MALACKA-DBN-SMALL&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The second DBN is the same expect INPUTS, F1 and F2 have 10000, 800,
800 neurons, respectively. See &lt;code&gt;MAKE-MALACKA-DBN-BIG&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The third DBN is the same expect INPUTS, F1 and F2 have 10000, 2000,
2000 neurons respectively. See &lt;code&gt;MAKE-MALACKA-DBN-BIGGER&lt;/code&gt;. Note that
this last DBN wasn&apos;t fine tuned due to time constraints; predictions
are extracted directly from the DBN, which doesn&apos;t try to minimize
cross entropy.&lt;/p&gt;

&lt;p&gt;The RBMs in the DBN were trained with contrastive divergence with
minibatches of 100 posts. Learning rate was 0.001, momentum 0.9,
weight decay 0.0002.&lt;/p&gt;

&lt;p&gt;The backprop networks were trained for 38 epochs with the conjugate
gradient method with three line searches on batches of 10000 posts.
For the first 5 epochs, only the softmax units were trained, and for
the last 3 epochs there was only one batch epoch (i.e. normal
conjugate gradient).&lt;/p&gt;

&lt;p&gt;These guys take several hours to days to train.&lt;/p&gt;

&lt;h3&gt;Logistic Regression&lt;/h3&gt;

&lt;p&gt;Not much to say here. I used liblinear with the top 250000 features,
with these parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:solver-type :l2r-lr
:c 256
:eps 0.001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although it had access to a much larger set of features, liblinear
could only achieve ~0.83 on the stratified sample used for
development vs ~0.79 for the second DBN. Even though they used the
same kind of features, their predictions were different enough to
produce a slightly better ensemble.&lt;/p&gt;

&lt;h3&gt;Vowpal Wabbit&lt;/h3&gt;

&lt;p&gt;I&apos;m not sure adding this helped at all in the end, the results
weren&apos;t entirely convincing. I just took Foxtrot&apos;s code. VW is run
with &lt;code&gt;--loss_function logistic --oaa 5&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;The Ensemble&lt;/h3&gt;

&lt;p&gt;The ensemble is a backpropagation neural network with one hidden
layer of 800 stochastic sigmoid neurons (at least that was the
intention, see below). The network looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PRED1 PRED2 PRED3 PRED4 PRED5
     _______|___/____/
            OUTPUT(800)
              |
         CROSS-ENTROPY
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PRED1 is made of five neurons representing the class probabilities
in the prediction of the first DBN. The rest of PRED* are for the
other two DBNs, the liblinear model, and VW.&lt;/p&gt;

&lt;p&gt;The network was trained with gradient descent with minibatches of
100 posts. The learning rate started out as 0.01 and multiplies by
0.98 each epoch. Momentum started out as 0.5 and was increased to
0.99 in 50 epochs. Learning rate was also multiplied by (1 -
momentum) to disentangle it from the momentum. No weight decay was
used.&lt;/p&gt;

&lt;p&gt;I tried to get Hinton&apos;s dropout technique working, but it didn&apos;t
live up to my expectations. On the other hand, the stochastic binary
neurons mentioned in the dropout presentation did help a tiny bit.
Unfortunately, I managed to make the final submission with a broken
version, where the weights of stochastic binary neurons were not
trained at all, effectively resulting in 800 random features (!).&lt;/p&gt;

&lt;h3&gt;Bagging&lt;/h3&gt;

&lt;p&gt;As good as stochastic binary neurons were before I broke the code,
it still helped a tiny bit (as in a couple of 0.0001s) to average 10
ensembles.&lt;/p&gt;

&lt;h2&gt;Additional Comments and Observations&lt;/h2&gt;

&lt;h3&gt;Time&lt;/h3&gt;

&lt;p&gt;It was clear from the beginning that time plays an important role,
and if scores are close, then predicting the class distribution of
the test set could be the deciding factor. I saw the pace of
change (with regards to the distribution of classes) picking up near
the end of the development training set and probed into the public
leaderboard by submitting a number different constant
predictions (the same prior for every post). It seemed that the last
two weeks or one month is best.&lt;/p&gt;

&lt;p&gt;There was no obvious seasonality or trend that could be exploited on
the scale of months. I checked whether Stackoverflow were changing
the mechanics but didn&apos;t find anything. I certainly didn&apos;t foresee
the drastic class distribution change that was to come.&lt;/p&gt;

&lt;h3&gt;Features&lt;/h3&gt;

&lt;p&gt;I tried a couple of feature extraction methods. The
Key-Substring-Group extractor looked very promising, but it simply
didn&apos;t scale to more than a thousand features.&lt;/p&gt;

&lt;p&gt;In the end, I found that no important features were left out by
playing with liblinear that could handle all features at the same
time. Take it with a grain of salt, of course, because there is a
signal/noise issue lurking.&lt;/p&gt;

&lt;h3&gt;Naive Bayes, Random Forests, Gradient Boosting&lt;/h3&gt;

&lt;p&gt;I experimented with the above in scikit-learn. The results were
terrible, but worse, they didn&apos;t contribute to the ensemble either.
Maybe it was only me.&lt;/p&gt;

&lt;h3&gt;Libsvm&lt;/h3&gt;

&lt;p&gt;I couldn&apos;t get it to scale to several tens of thousands posts, so I
had to go with liblinear.&lt;/p&gt;

&lt;h3&gt;Dropout&lt;/h3&gt;

&lt;p&gt;Fine tuning DBNs with dropout or stochastic binary neurons (without
the bugs) didn&apos;t work. The best I could achive was slightly worse
than the conjugate gradient based score.&lt;/p&gt;

&lt;h3&gt;Retraining Constituent Models&lt;/h3&gt;

&lt;p&gt;Recall that the constituent models were trained only on 4/5 of the
available data. After the ensemble was trainined, I intended to
retrain them on the whole stratified training set. Initial
experiments with liblinear were promising, but with the DBN the
public leaderboard score got a lot worse and I ran out of time to
experiment.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Tue, 09 Apr 2013 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>Alpha–Beta</title>
            <link>http://quotenil.com/alpha-beta.html</link>
            <description>&lt;p&gt;It hasn&apos;t even been a year yet since I first promised that alpha–beta
snippet, and it is already added to Micmac in all its &lt;a href=&quot;https://github.com/melisgl/micmac/blob/ea5f6aa2b16be54f6c83a514d9aec223a00baf92/src/graph-search.lisp#L9&quot; &gt;35 line
glory&lt;/a&gt;.
The good thing about not rushing it out the door is that it saw a
bit more use. For a tutorialish tic-tac-toe example see
&lt;a href=&quot;https://github.com/melisgl/micmac/blob/ea5f6aa2b16be54f6c83a514d9aec223a00baf92/test/test-alpha-beta.lisp&quot; &gt;test/test-game-theory.lisp.&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The logging code in the example produces
&lt;a href=&quot;blog-files/alpha-beta-log.png&quot; &gt;output&lt;/a&gt;, which is suitable for cut
and pasting into an org-mode buffer and exploring it by &lt;code&gt;TAB&lt;/code&gt;bing
into subtrees to answer the perpetual &apos;What the hell was it
thinking?!&apos; question.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Mon, 27 Dec 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Nash Equilibrium Finder</title>
            <link>http://quotenil.com/nash-equilibrium-finder.html</link>
            <description>&lt;p&gt;While I seem to be unable to make my mind up on a good interface to
alpha–beta with a few bells and whistles, I added a Nash equilibrium
finder to &lt;a href=&quot;http://cliki.net/micmac&quot; &gt;Micmac&lt;/a&gt;, which is becoming less
statistics oriented. This was one of the many things in Planet Wars
that never really made it.
Let&apos;s consider the &lt;a href=&quot;http://en.wikipedia.org/wiki/Matching_pennies&quot; &gt;Matching
pennies&lt;/a&gt; game. The
row player wins iff the two pennies show the same side. The payoff
matrix is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;|       | Heads | Tails |
+-------+-------+-------+
| Heads |     1 |    -1 |
| Tails |    -1 |     1 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Find the mixed strategy equilibrium:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;find-nash-equilibrium &apos;&lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-1 1&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;1 -1&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
=&amp;gt;
#&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;49 51&lt;/span&gt;)&lt;/span&gt;
#&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;50 50&lt;/span&gt;)&lt;/span&gt;
-0.01&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is, both players should choose heads 50% of the time and the
expected payoff (for the row player) is zero of which -0.01 is an
approximation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;find-nash-equilibrium &apos;&lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-1 1&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;1 -1&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:n-iterations&lt;/span&gt; 1000&lt;/span&gt;)&lt;/span&gt;
=&amp;gt;
#&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;499 501&lt;/span&gt;)&lt;/span&gt;
#&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;500 500&lt;/span&gt;)&lt;/span&gt;
-0.001&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Sun, 26 Dec 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Planet Wars Post-Mortem</title>
            <link>http://quotenil.com/planet-wars-post-mortem.html</link>
            <description>&lt;p&gt;I can&apos;t believe I &lt;a href=&quot;https://web.archive.org/web/20101205003152/http://ai-contest.com/rankings.php&quot; &gt;won&lt;/a&gt;.&lt;br&gt;
I can&apos;t believe I won &lt;em&gt;decisively&lt;/em&gt; at all.&lt;/p&gt;

&lt;p&gt;The lead in the last month or so was an indicator of having good
chances, but there was a huge shuffling of ranks in the last week
and some last minute casualties.&lt;/p&gt;

&lt;h2&gt;Code&lt;/h2&gt;

&lt;p&gt;Note that the git repository is available at
[https://github.com/melisgl/planet-wars(https://github.com/melisgl/planet-wars).&lt;/p&gt;

&lt;h2&gt;Denial&lt;/h2&gt;

&lt;p&gt;I had promised myself not to enter this one and resisted for about
two weeks when my defenses were worn away and I was drawn into the
fray.&lt;/p&gt;

&lt;p&gt;The game didn&apos;t look very exciting at first. I thought that the bots
would soon reach a point of near perfect tactics and the
rock-paper-scissors scenarios would dominate (more on this later).&lt;/p&gt;

&lt;p&gt;That&apos;s enough of
&lt;a href=&quot;https://web.archive.org/web/20100307014152/http://www.a1k0n.net/blah/archives/2010/03/index.html&quot; &gt;tribute&lt;/a&gt;,
let&apos;s steer off the trodden path.&lt;/p&gt;

&lt;h2&gt;Beginning&lt;/h2&gt;

&lt;p&gt;Driven by the first
&lt;a href=&quot;http://en.wikipedia.org/wiki/Larry_Wall#Virtues_of_a_programmer&quot; &gt;virtue&lt;/a&gt;
of programmers I was going to approach the game in a
non-labor-intensive fashion leaving most of the hard work to the
machine. The second virtue was kept in check for a week while I was
working out how to do that exactly. In the meantime, the third
spurred me to take &lt;a href=&quot;http://aerique.blogspot.com/&quot; &gt;aerique&apos;s&lt;/a&gt; starter
pack and to make myself comfortable with minor modifications to it.&lt;/p&gt;

&lt;p&gt;As with tron, UCT was on my mind. However, I was keenly aware of
failing to make it work acceptably last time. No matter how cool UCT
was, it was hard to miss one important similarity to tron: the
fitness function is very jagged, one ship more or less can make all
the difference. Clearly, a naive random policy was not going to cut
it.&lt;/p&gt;

&lt;p&gt;Another problem was the practically unlimited branching factor.
Without a similarity function over moves, it was hopeless to explore
a meaningful portion of the game tree.&lt;/p&gt;

&lt;h2&gt;Move Generation&lt;/h2&gt;

&lt;p&gt;At this point I had to start getting my hands dirty. The first thing
was to implement simulating the future (see &lt;code&gt;FUTURE&lt;/code&gt; class), which
was trivial except I screwed battle resolution up and for the
longest time it was holding results back. Think of a future as a
vector of owner and ship count over turns.&lt;/p&gt;

&lt;p&gt;By watching some games, it became apparent that multi-planet,
synchronized attacks are the way to go. The implementation operates
on step targets, steps and moves.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;step&lt;/em&gt; is a set of orders from the same player targeting the same
planet. The constituent orders need not be for the same turn,
neither do they need to arrive on the same turn.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;move&lt;/em&gt; is a set of orders from the same player without any
restriction. That includes future orders too.&lt;/p&gt;

&lt;p&gt;Move generation first computes so called step targets. A &lt;em&gt;step target&lt;/em&gt;
is a ship count vector over turns representing the desired arrivals.
The desired arrivals are simply minimal reinforcements for defense and
invasion forces for attack.&lt;/p&gt;

&lt;p&gt;For each step target a number of steps can be found that produce the
desired arrivals. In the current implementation, there is a single
step generated for a step target.&lt;/p&gt;

&lt;p&gt;For a while my bot could only make moves that consisted of a single
step, but it quickly became the limiting factor, and strength
testing of modifications was impossible.&lt;/p&gt;

&lt;p&gt;Combining steps into moves turned out to be easy. Not all
combinations are valid, but the number of combinations can be huge.
To limit the number of moves generated, we first evaluate steps one
by one, sort them in descending order of evaluation score and try to
combine them starting from the first.&lt;/p&gt;

&lt;h2&gt;Full Attack&lt;/h2&gt;

&lt;p&gt;Normally, futures are calculated taking into account fleets already
in flight in the observable game state that the engine sends. Back
when I was still walking up and down instead of typing away
furiously, it occurred to me that if for all planets of player 1,
player 2 cannot take that planet if both players sent all ships to
it, then player 2 cannot take any planet of player 1 even if he&apos;s
allowed to attack multiple planets in any pattern. Clearly, this
breaks down at the edges (simultaneous moves), but it was a useful
idea that gave birth to the &lt;code&gt;FULL-ATTACK-FUTURE&lt;/code&gt; class. The
intention was to base position evaluation on the sum of scores of
individual full attack futures (one per planet).&lt;/p&gt;

&lt;p&gt;Now the problem with full attack future is that sending all
available ships away from a planet can invalidate some orders
scheduled from that planet for the future. Enter the concept (and
class) of &lt;code&gt;SURPLUS&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The surplus of player P at planet A at time t is the number of ships
that can be sent away on that turn from the defending army without:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;making any scheduled order from planet A invalid&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;causing the planet to be lost anytime after that (observing only
  the fleets already in space)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;bringing an imminent loss closer in time&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As soon as the full attack based position evaluation function was
operational, results started to come. But there was a crucial
off-by-one bug.&lt;/p&gt;

&lt;h2&gt;Constraining Futures&lt;/h2&gt;

&lt;p&gt;That bug was in the scoring of futures. For player 1, it used the
possible arrivals (number of ships) one turn before those of player
2. I made several attempts at fixing it, but each time playing
strength dropped like a stone.&lt;/p&gt;

&lt;p&gt;Finally, a principled solution emerged: when computing the full
attack future from the surpluses, constrain the turn of departures.
That is, to roughly duplicate the effect of the off-by-one bug, one
could say that surpluses of player 1 may not leave the planet before
turn 1 (turn 0 is current game state) (see &lt;code&gt;MIN-TURN-TO-DEPART-1&lt;/code&gt; in
the code). This provided a knob to play with. Using 1 for
&lt;code&gt;MIN-TURN-TO-DEPART-1&lt;/code&gt; made the bot actually prefer moves to just
sitting idly, using 2 made it prefer moves that needed no
reinforcement on the next turn.&lt;/p&gt;

&lt;p&gt;I believe the following is the most important one character change I
made, so this gets its own paragraph. Using 2 as
&lt;code&gt;MIN-TURN-TO-DEPART-1&lt;/code&gt; makes the bot tend towards situations in
which the rock-paper-scissors nature of the game is suppressed. The
same bot with 1 beats the one with 2, but as was often the case, on
TCP the results were just the opposite. By a big margin. TCP is
dhartmei&apos;s unofficial server, where most useful testing took place.&lt;/p&gt;

&lt;p&gt;Constraints were added for arrivals too (see &lt;code&gt;MIN-TURN-TO-ARRIVE&lt;/code&gt;),
which eased scoring planets that started out neutral but were
non-neutral at the horizon by making the evaluation function sniping
aware.&lt;/p&gt;

&lt;p&gt;Sniping is when one player takes a neutral, losing ships in the
process, and the opponent comes – typically on the next turn – and
takes it away. &lt;a href=&quot;https://web.archive.org/web/20101213023101/http://www.ai-contest.com/visualizer.php?game_id=9347535&quot; &gt;This
game&lt;/a&gt;
is a nice illustration of the concept.&lt;/p&gt;

&lt;h2&gt;Redistribution&lt;/h2&gt;

&lt;p&gt;As pointed out by
&lt;a href=&quot;https://web.archive.org/web/20110210212413/http://iouri-khramtsov.blogspot.com/2010/11/google-ai-challenge-planet-wars-entry.html&quot; &gt;iouri&lt;/a&gt;
in his post-mortem, redistribution of ships is a major factor. The
machinery described so far lends itself to easy implementation of
redistribution.&lt;/p&gt;

&lt;p&gt;When scoring a full attack future, the scoring function gives a very
slight positional penalty every simulated turn for every enemy ship.
This has the effect of preferring positions where the friendly ships
are near the enemy, and positions of influence with multiple enemy
planets being threatened.&lt;/p&gt;

&lt;p&gt;The move generator was modified to generate steps to each friendly
planet from each friendly planet on turn 0 sending all the surplus
at that turn. This scheme is rather restrictive, the more flexible
solutions had mixed results.&lt;/p&gt;

&lt;p&gt;There is a knob, of course, to control how aggressively ships are
redistributed. It&apos;s called &lt;code&gt;POSITIONAL-MIN-TURN-TO-DEPART-1&lt;/code&gt;. As its
name implies it&apos;s like &lt;code&gt;MIN-TURN-TO-DEPART-1&lt;/code&gt; but used only when
computing the positional penalty.&lt;/p&gt;

&lt;h2&gt;Dynamic Horizon&lt;/h2&gt;

&lt;p&gt;How far ahead the bot looks has a very strong effect on its play:
too far and it will be blind to tactics, too close and it will miss
capturing higher cost neutrals.&lt;/p&gt;

&lt;p&gt;Horizon was constant 30 for quite some time. I wanted to raise it
but couldn&apos;t without seriously hurting close range fighting ability.
After much experimentation with a slightly complicated mechanism the
horizon was set so that the three earliest breakeven turns of safe
to take neutrals are included. A neutral is deemed safe to take if
from the initial investment until the breakeven point no friendly
planet can be possibly lost in a full attack future.&lt;/p&gt;

&lt;h2&gt;Nash Equilibrium&lt;/h2&gt;

&lt;p&gt;There are – especially at the very beginning of games – situations
where there is no best move, it all depends on what the opponent
plays on the same turn.&lt;/p&gt;

&lt;p&gt;If one has a number of candidate moves for each player and the score
for any pair of them, the optimal mixed strategy can be computed,
which is just a probability assigned to each move.&lt;/p&gt;

&lt;p&gt;I tried and tried to make it work, but it kept making mistakes that
looked easy to exploit, and although it did beat 1 ply minimax about
2 to 1 it was too slow to experiment with.&lt;/p&gt;

&lt;h2&gt;Alpha–Beta&lt;/h2&gt;

&lt;p&gt;Yes, for the longest time, it was a 1-ply search. Opponent moves
were never considered, and position evaluation was good enough to
pick up the slack.&lt;/p&gt;

&lt;p&gt;However, there was a problem. The evaluation function did not score
planets that were neutral at the end of the normal future, because
doing so made the bot just sit there doing nothing, getting high
scores for all planets that could be conquered, but when it tried to
make a move it realized that it can conquer only one. Such is the
nature of full attack based evaluation function, it was designed
with complete disregard for neutrals.&lt;/p&gt;

&lt;p&gt;A late change to the map generator increased the number of planets
at an equal distance from the players and emphasized the
rock-paper-scissors nature further. Some bots didn&apos;t like it, some
took this turn of events better. Before this point, my bot had a
very comfortable lead on the official leaderboard, which was greatly
reduced.&lt;/p&gt;

&lt;p&gt;With the failure of the Nash experiment, I resurrected previously
unsuccessful alpha–beta code in hopes of that considering opponent
moves will show the bot the error of it ways, and force it to not
leave valuable central planets uncovered.&lt;/p&gt;

&lt;p&gt;It&apos;s tricky to make alpha–beta work with moves that consist of
orders at arbitrary times in the future. I had all kinds of funky,
correct and less correct ways to execute orders at different depths
of the search. In the end, what prevailed was the most
simple-minded, incorrect variant that simply scheduled all orders
that made up the move (yes, even the future ones) and fixed things
up when computing the future so that ship counts stayed non-negative
and sending enemy ships did not occur.&lt;/p&gt;

&lt;p&gt;In local tests against older versions of my bot, a two ply
alpha–beta bot showed very promising results, but when it was tested
on tcp it fell way short of the expectations and performed worse
than the one ply bot. It seemed particularly vulnerable to a number
of bots. In retrospect, I think this was because their move
generator was sufficiently different that my bot was just blind to a
good range of real possibilities.&lt;/p&gt;

&lt;p&gt;In the end, I settled for using four ply alpha–beta for the opening
phase (until the third planet was captured). This allowed the bot to
outwait opponents when needed and win most openings. After the final
submission, I realized that maybe I was trying to push things the
wrong way and even three planets is too many. With six hours left
until the deadline, in a test against binaries of a few fellow
competitors the two planet limit seemed to perform markedly better,
but it was too late to properly test it against a bigger population.&lt;/p&gt;

&lt;h2&gt;The End&lt;/h2&gt;

&lt;p&gt;Like many fellow contestants, I am very happy that the contest is
over and I got my life back. I&apos;m sure that many families breathed a
collective sigh of relief. But if I were to continue, I&apos;d try
rethinking the move generator because that may just be the thing
that holds alpha–beta back and maybe Nash too.&lt;/p&gt;

&lt;p&gt;Dissapointingly, there was no learning, adapting to opponent
behaviour, etc. All that made it to the todo list but had to take
second seat to more pressing concerns.&lt;/p&gt;

&lt;p&gt;Ah, yes. One more thing. Bocsimack&#xF3; (pronounced roughly as
bo-chee-mats-ko), after whom the bot was named, is the handsome hero
of a children&apos;s book, pictured on the left:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/malacka-es-bocsimacko.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Wed, 01 Dec 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Important Update to the Planet Wars Starter Package</title>
            <link>http://quotenil.com/important-update-to-the-planet-wars-starter-package.html</link>
            <description>&lt;p&gt;First, is it possible to get something as simple
as &lt;code&gt;RESOLVE-BATTLE&lt;/code&gt; wrong? Apparently, yes. That&apos;s what one gets for
trying to port Python code that&apos;s pretty foreign in the sense of
being far from the way I&apos;d write it.
More importantly, I found out the hard way that sbcl 1.0.11, that&apos;s
&lt;a href=&quot;http://code.google.com/p/ai-contest/issues/detail?id=183&quot; &gt;still&lt;/a&gt; on
the official servers, has a number of bugs in its timer
implementation making &lt;code&gt;WITH-TIMEOUT&lt;/code&gt; unreliable. Also, it can
trigger timeouts recursively, eventually exceeding the maximum
interrupt nesting depth. Well, &amp;quot;found out&amp;quot; is not the right way to
put it as we did fix most of these bugs ages ago.&lt;/p&gt;

&lt;p&gt;In the new starter package (v0.8 in
&lt;a href=&quot;http://quotenil.com/git/?p=planet-wars.git;a=summary&quot; &gt;git&lt;/a&gt;, &lt;a href=&quot;http://quotenil.com/binary/planet-wars/planet-wars-latest.tar.gz&quot; &gt;latest
tarball&lt;/a&gt;),
you&apos;ll find a timer.lisp that&apos;s simply backported almost verbatim from
sbcl 1.0.41 to sbcl 1.0.11. Seems to work for me, but I also had to
lower the timeout to 0.8 from 0.98 because the main server is
extremely slow.&lt;/p&gt;

&lt;p&gt;The rate at which games are played on the servers is so low that it
takes several days to ascend through the leaderboard. Nevertheless,
an old buggy version is sitting on the
&lt;a href=&quot;https://web.archive.org/web/20101025070429/http://ai-contest.com/rankings.php&quot; &gt;top&lt;/a&gt;
right now. Mind you, introducing bugs is a great way exlopore the
solution space, and it&apos;s quite worrisome just how adept I am at this
poor man&apos;s evolutionary programming. Most of them have since been
fixed while the ideas they brought to light remain, making the
current version much stronger.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Mon, 25 Oct 2010 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>Planet Wars Common Lisp Starter Package Actually Works</title>
            <link>http://quotenil.com/planet-wars-common-lisp-starter-package-that-actually-works.html</link>
            <description>&lt;p&gt;Released
v0.6 (&lt;a href=&quot;http://quotenil.com/git/?p=planet-wars.git;a=summary&quot; &gt;git&lt;/a&gt;,
&lt;a href=&quot;http://quotenil.com/binary/planet-wars/planet-wars-latest.tar.gz&quot; &gt;latest
tarball&lt;/a&gt;).
The way the server compiles lisp submissions was fixed, and this
revealed a problem where MyBot.lisp redirected &lt;a href=&quot;http://www.lispworks.com/documentation/HyperSpec/Body/v_debug_.htm&quot; title=&quot;*STANDARD-OUTPUT* VARIABLE&quot;&gt;&lt;code&gt;*STANDARD-OUTPUT*&lt;/code&gt;&lt;/a&gt;
to &lt;a href=&quot;http://www.lispworks.com/documentation/HyperSpec/Body/v_debug_.htm&quot; title=&quot;*ERROR-OUTPUT* VARIABLE&quot;&gt;&lt;code&gt;*ERROR-OUTPUT*&lt;/code&gt;&lt;/a&gt; causing the server to think compilation failed.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Tue, 21 Sep 2010 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>Planet Wars Common Lisp Starter Package</title>
            <link>http://quotenil.com/planet-wars-common-lisp-starter-package.html</link>
            <description>&lt;p&gt;The &lt;a href=&quot;https://web.archive.org/web/20100926070007/http://ai-contest.com/&quot; &gt;Google AI
Challenge&lt;/a&gt;
is back with a new game that&apos;s supposed to be much harder than Tron
was this spring. The branching factor of the game tree is enormous,
which only means that straight minimax is out of question this time
around. Whether some cleverness can bring the game within reach of
conventional algorithms remains to be seen.
Anyway, I&apos;m adding &lt;a href=&quot;http://quotenil.com/git/?p=planet-wars.git;a=summary&quot; &gt;yet another starter
package&lt;/a&gt; (&lt;a href=&quot;http://quotenil.com/binary/planet-wars/planet-wars-latest.tar.gz&quot; &gt;latest
tarball&lt;/a&gt;)
to the
&lt;a href=&quot;http://aerique.blogspot.com/2010/09/planet-wars-common-lisp-start-package.html&quot; &gt;lot&lt;/a&gt;.
It is based heavily on aerique&apos;s.&lt;/p&gt;

&lt;p&gt;Highlights compared to his version:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;no excessive use of specials (&lt;code&gt;*INPUT*&lt;/code&gt;, &lt;code&gt;*FLEETS*&lt;/code&gt;, etc)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;player class to support different types of players&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;MyBot.lisp&lt;/code&gt; split into several files&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;it uses asdf (more convenient development)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;made it easier to run tests with executables (&lt;code&gt;./MyBot&lt;/code&gt;) or when
  starting a fresh sbcl (&lt;code&gt;./bin/run-bot.sh&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Proxy bot server:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;can run compiled (&lt;code&gt;./ProxyBot&lt;/code&gt;) or &lt;code&gt;./bin/run-proxy-bot.sh&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;started explicitly (no &lt;code&gt;:PWBOT-LOCAL&lt;/code&gt; reader magic)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;can serve any number of proxy bots&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;closes sockets properly&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is still a
&lt;a href=&quot;https://web.archive.org/web/20110709095120/http://ai-contest.com/forum/viewtopic.php?f=18&amp;t=421&amp;start=40&quot; &gt;problem&lt;/a&gt;
causing all lisp submissions to die on the first turn no matter
which starter package one uses, which will hopefully be resolved.
Until then there is dhartmei&apos;s excellent &lt;a href=&quot;https://web.archive.org/web/20100926103520/http://ai-contest.com/forum/viewtopic.php?f=18&amp;t=424&quot; &gt;unofficial tcp
server&lt;/a&gt;.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Sun, 19 Sep 2010 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>UCT</title>
            <link>http://quotenil.com/uct.html</link>
            <description>&lt;p&gt;As promised, my &lt;a href=&quot;http://senseis.xmp.net/?UCT&quot; &gt;UCT&lt;/a&gt;
implementation is released, albeit somewhat belatedly. It&apos;s in
&lt;a href=&quot;http://cliki.net/Micmac&quot; &gt;Micmac&lt;/a&gt; v0.0.1, see &lt;code&gt;test/test-uct.lisp&lt;/code&gt;
for an example. Now I only owe you alpha–beta.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Fri, 19 Mar 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Google AI Challenge 2010 Results</title>
            <link>http://quotenil.com/google-ai-challenge-2010-results.html</link>
            <description>&lt;p&gt;For what has been a fun ride, the official results are now &lt;a href=&quot;https://web.archive.org/web/20110724100751/http://csclub.uwaterloo.ca/contest/rankings.php&quot; &gt;available&lt;/a&gt;.
In the end, 11th out of 700 is not too bad and it&apos;s the highest
ranking non-C++ entry by some margin.
I entered the contest a bit late with a rather specific approach in
mind: &lt;a href=&quot;http://senseis.xmp.net/?UCT&quot; &gt;UCT&lt;/a&gt;, an algorithm from the
Monte Carlo tree search family. It has been rather successful in
Go (and in Hex too, taking the crown from
&lt;a href=&quot;hex/six/index.html&quot; &gt;Six&lt;/a&gt;). So with UCT in mind, to serve as a
baseline, I implemented a quick
&lt;a href=&quot;http://en.wikipedia.org/wiki/Minimax&quot; &gt;minimax&lt;/a&gt; with a simple
territory based evaluation function ... that everyone else in the
competition seems to have invented independently. Trouble was
looming because it was doing too well: with looking ahead only one
move (not even considering moves of the opponent), it played a very
nice positional game. That was the first sign that constructing a
good evaluation function may not be as hard for Tron as it is for
Go.&lt;/p&gt;

&lt;p&gt;But with everyone else doing minimax, the plan was to keep silent
and Monte Carlo to victory. As with most plans, it didn&apos;t quite work
out. First, to my dismay, some contestants were attempting to do the
same and kept advertising it on &lt;code&gt;#googleai&lt;/code&gt;, second it turned out
that UCT is not suited to the game of Tron. A totally random default
policy kept cornering itself in a big area faster than another
player could hit the wall at the end of a long dead end. That was
worrisome but fixable. After days of experimentation I finally gave
up on it deciding that Tron is simply too tactical – or not fuzzy
enough, if you prefer – for MC to work really well.&lt;/p&gt;

&lt;p&gt;Of course, it can be that the kind of default policies I tried were
biased (a sure thing), misguided and suboptimal. But then again, I
was not alone and watched the UCT based players struggle badly. In
the final standings the highest ranking one is jmcarthur in position
105. One of them even implemented a number of different default
policies and switched between them randomly with little apparent
success. Which makes me think that including a virtual strategy
selection move at some points in the UCT search tree should be
interesting, but I digress.&lt;/p&gt;

&lt;p&gt;So I went back to minimax, implemented &lt;a href=&quot;http://en.wikipedia.org/wiki/Alpha-beta_pruning&quot; &gt;alpha–beta
pruning&lt;/a&gt; with
principal variation, and &lt;a href=&quot;http://en.wikipedia.org/wiki/Iterative_deepening&quot; &gt;iterative
deepening&lt;/a&gt;. It
seemed to do really well on the then current maps whose size was
severely reduced to 15x15 to control the load on the servers. Then,
I had an idea to explore how the parities of squares in an area
affect the longest path possible, which was quickly pointed out to
me over lunch by a friend. And those pesky competitors have also
found and advertised it in the contest forum. Bah.&lt;/p&gt;

&lt;p&gt;There were only two days left at this point, and I had to pull an
all nighter to finally implement a graph partitioning idea of mine
that unsurprisingly someone has described pretty closely in the
forum. At that point, I finally had the tool to improve the
evaluation function but neither much time or energy remained and I
settled for using it only in the end game when the players are
separated.&lt;/p&gt;

&lt;p&gt;The code itself is as ugly as exploratory code can be, but in the
coming days, I&apos;ll factor the UCT and the alpha–beta code out.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Mon, 01 Mar 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Google AI Challenge 2010</title>
            <link>http://quotenil.com/google-ai-challenge-2010.html</link>
            <description>&lt;p&gt;Tron is a fun little game of boxing out the opponent and avoiding
crashing into a wall first. The rules are simple, so the barrier to
entry into &lt;a href=&quot;https://web.archive.org/web/20100207135122/http://csclub.uwaterloo.ca/contest/index.php&quot; &gt;this
contest&lt;/a&gt;
is low. Thanks to &lt;a href=&quot;http://www.aerique.net/&quot; &gt;aeruiqe&lt;/a&gt;, who made the
Common Lisp starter pack, it took as little as a few hours to get a
very bare-bones algorithm going. It&apos;s doing surprisingly well: it is
number 23 on the
&lt;a href=&quot;https://web.archive.org/web/20110724100751/http://csclub.uwaterloo.ca/contest/rankings.php&quot; &gt;leaderboard&lt;/a&gt;
at the moment with 43 wins, 2 losses and 9 draws.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Thu, 11 Feb 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Micmac Initial Release</title>
            <link>http://quotenil.com/micmac-initial-release.html</link>
            <description>&lt;p&gt;From a failed experiment today, I salvaged
&lt;a href=&quot;http://cliki.net/micmac&quot; &gt;Micmac&lt;/a&gt;, a statistical library wannabe,
which for now only has Metropolis-Hastings MCMC and Metropolis
Coupled MCMC implemented. The code doesn&apos;t weigh much, but I think
it gets the API right. In other news &lt;a href=&quot;http://cliki.net/MGL&quot; &gt;MGL&lt;/a&gt;
v0.0.6 was released.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Sat, 06 Feb 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Deep Boltzmann Machine on MNIST</title>
            <link>http://quotenil.com/deep-boltzmann-machine-on-mnist.html</link>
            <description>&lt;p&gt;Let me interrupt the flow of the &lt;a href=&quot;http://cliki.net/MGL&quot; &gt;MGL&lt;/a&gt;
introduction series with a short report on what I learnt playing
with &lt;a href=&quot;http://www.cs.toronto.edu/~hinton/absps/dbm.pdf&quot; &gt;Deep Boltzmann
Machines&lt;/a&gt;. First,
lots of thanks to Ruslan Salakhutdinov, then at &lt;a href=&quot;http://www.cs.toronto.edu/~rsalakhu/&quot; &gt;University of
Toronto&lt;/a&gt; now at
&lt;a href=&quot;http://web.mit.edu/~rsalakhu/www/&quot; &gt;MIT&lt;/a&gt;, for making the Matlab
source &lt;a href=&quot;http://web.mit.edu/~rsalakhu/www/DBM.html&quot; &gt;code&lt;/a&gt; for the
&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot; &gt;MNIST&lt;/a&gt; digit classification
problem available.
The linked &lt;a href=&quot;http://www.cs.toronto.edu/~hinton/absps/dbm.pdf&quot; &gt;paper&lt;/a&gt;
claims a record of 99.05% in classification accuracy on the
permutation invariant task (no prior knowledge of geometry). A
previous approach trained a
&lt;a href=&quot;http://www.scholarpedia.org/article/Deep_belief_networks&quot; &gt;DBN&lt;/a&gt; in
an unsupervised manner and fine tuned it with backpropagation. Now,
there is one more step: turning the DBN into a DBM (Deep Boltzmann
Machine) and tune it further before handing the baton over to
backprop. While in a DBN the constituent RBMs are trained one by
one, the DBM is trained as a whole which, in theory, allows it to
reconcile bottom-up and top-down signals, i.e. what it sees and what
it thinks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/mnist-2-dbm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the diagram above, as before, dark gray boxes are constants (to
provide the connected chunks with biases), inputs are colored mid
gray while hidden features are light gray. &lt;code&gt;INPUTS&lt;/code&gt; is where the
28x28 pixel image is clamped and &lt;code&gt;LABEL&lt;/code&gt; is a softmax chunk for the
10 digit classes.&lt;/p&gt;

&lt;p&gt;In the Matlab code, there are a number of prominent features that
may or may not be important to this result:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The second RBM gets the the correct label as input which
  conveniently allows tracking classification accuracy during its
  training but also – more importantly – forces the top-level
  features to be somewhat geared towards reconstruction of labels
  and thus classification.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A sparsity term is added to the gradient. Sparse representations
  are often better for classification.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Focusing only on what makes DBM learning tick, I tried a few
variants of the basic approach. All of them start with the same DBN
whose RBMs are trained for 100 epochs each:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/mnist-2-dbn-training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DBN training finishes with around 97.77%, averaging 97.9% in the
last 10 epochs.&lt;/p&gt;

&lt;p&gt;On to the DBM. As the baseline, the DBM was not trained at all and
the BPN did not get the marginals of the approximate posterior as
inputs as prescribed in the paper, only the normal input. It&apos;s as if
the DBN were unrolled into a BPN directly. Surprisingly, this
baseline is already at 99.00% at the end of BPN training (all
reported accuracies are averages from the last 10 epochs of
training).&lt;/p&gt;

&lt;p&gt;The second variant performs DBM training but without any sparsity
term and gets 99.07%. The third is using a sparsity
penalty (\&amp;quot;normal sparsity\&amp;quot; in the diagram) for units in opposing
layers on at the same time and nets 99.08%. The fourth is just a
translation of the sparsity penalty from the Matlab code. This one
is named &amp;quot;cheating sparsity&amp;quot; because it – perhaps in an effort to
reduce variance of the gradient – changes weights according to the
average activation levels of units connected by them. Anyway, this
last one reaches 99.09%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/mnist-2-dbm-training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/mnist-2-bpn-training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To reduce &lt;a href=&quot;http://en.wikipedia.org/wiki/Publication_bias&quot; &gt;publication
bias&lt;/a&gt; a bit, let me
mention some experiments that were found to have no effect:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In an effort to see whether DBM training is held back by high
  variance of the gradient estimates a batch size of 1000 (instead
  of 100) was tested for a hundred epochs after the usual 500. There
  was no improvement.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the BPN, label weights and biases were initialized from the
  DBM. This initial advantage diminishes gradually and by the end of
  training there is nothing (+0.01%) between the initialized and
  uninitialized variants. Nevertheless, all results and diagrams are
  from runs with label weights initialized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The matlab code goes out of its way to compute negative phase
  statistics from the &lt;em&gt;expectations&lt;/em&gt; of the units in &lt;code&gt;F1&lt;/code&gt; and &lt;code&gt;F2&lt;/code&gt;
  supposedly to help with variance of estimates and this turned out
  to be very important: with the same calculation based on the
  sampled values DBM classification deteriorated. Using the
  expectations for chunks &lt;code&gt;INPUTS&lt;/code&gt; and &lt;code&gt;LABEL&lt;/code&gt; did not help, though.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What I take home from these experiments is that from the
considerable edge of DBM over DBN training only a small fraction
remains by the end of BPN training and that the additional sparsity
constraint accounts for very little in this setup.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Mon, 18 Jan 2010 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Introduction to MGL (part 3)</title>
            <link>http://quotenil.com/introduction-to-mgl-part-3.html</link>
            <description>&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: This post out of date with regards to current MGL.
Please refer to the
&lt;a href=&quot;http://melisgl.github.io/mgl-pax-world/mgl-manual.html&quot; &gt;documentation&lt;/a&gt;
instead.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;@INTRODUCTION-TO-MGL-PART-2&lt;/code&gt;, we went through a trivial example of
a backprop network. I said before that the main focus is on
Boltzmann Machines so let&apos;s kill the suspense here and now by
cutting straight to the heart of the matter.
&lt;a href=&quot;http://cseweb.ucsd.edu/users/gary/pubs/cottrell-science-2006.pdf&quot; &gt;Cottrell&apos;s Science
article&lt;/a&gt;
provides a clear and easy to follow description of the spiral
problem that we are going to implement. The executive summary is
that we want to train an auto-encoder: a network that reproduces its
input as output with a small encoding layer somewhere in between. By
forcing the information through the bottleneck of the encoding layer
the network should pick up a low dimensional code that represents
the input, thus performing dimensionality reduction.&lt;/p&gt;

&lt;p&gt;The function under consideration is &lt;code&gt;f(x)&lt;/code&gt;[x, sin(x), cos(x)]&lt;code&gt;. It
is suprisingly difficult to learn the mapping from&lt;/code&gt;x&lt;code&gt;to&lt;/code&gt;f(x)`. A
network architecture that is able to represent this transformation
has 3 inputs, 10 neurons in the next layer, 1 neuron in the encoding
layer, 10 neurons again in the reconstruction part and 3 in the
output layer. However, randomly initialized backpropagation fails at
learning this; a better solution is to first learn a Deep Belief
Network, \&amp;quot;unroll\&amp;quot; it to a backprop network and use backprop to
fine tune the weights.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;http://www.scholarpedia.org/article/Boltzmann_machine#Learning_deep_networks_by_composing_restricted_Boltzmann_machines&quot; &gt;Deep Belief
Network&lt;/a&gt;
is just a stack of &lt;a href=&quot;http://www.scholarpedia.org/article/Boltzmann_machine#Restricted_Boltzmann_machines&quot; &gt;Restricted Boltzmann
Machines&lt;/a&gt;.
An RBM is a BM restricted to be a two layer network with no
intralayer connections. The lower layer is called the visible, and
the higher layer is called hidden layer because from the point of
view of a single RBM, it is the visible layer that&apos;s connected to –
maybe indirectly – to external stimuli. In the upward pass of a DBN,
where the low level representations are subsequently transformed
into higher level ones by the constituent RBMs, the values of the
hidden units are clamped onto the visible units of the next RBM. In
other words, an RBM shares its visible and hidden layers with the
hidden and visible layers of the RBM below and above, respectively,
respectively.&lt;/p&gt;

&lt;p&gt;Let&apos;s start with a few utility functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; sample-spiral &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;random &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;flt &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;* 4 pi&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; make-sampler &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;n&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;counting-function-sampler
                 &lt;span class=&quot;keyword&quot;&gt;:max-n-samples&lt;/span&gt; n
                 &lt;span class=&quot;keyword&quot;&gt;:sampler&lt;/span&gt; #&apos;sample-spiral&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; clamp-array &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;x array start&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;setf &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;aref array &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;+ start 0&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt; x
        &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;aref array &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;+ start 1&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;sin x&lt;/span&gt;)&lt;/span&gt;
        &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;aref array &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;+ start 2&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;cos x&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; clamp-striped-nodes &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;samples striped&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;nodes &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;storage &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;nodes striped&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;loop&lt;/span&gt;&lt;/i&gt; for sample in samples
          for stripe upfrom 0
          do &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;with-stripes&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;stripe striped start&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
               &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;clamp-array sample nodes start&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Subclass &lt;code&gt;RBM&lt;/code&gt; and define &lt;code&gt;SET-INPUT&lt;/code&gt; using the above utilites:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defclass&lt;/span&gt;&lt;/i&gt; spiral-rbm &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;rbm&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defmethod&lt;/span&gt;&lt;/i&gt; mgl-train:set-input &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;samples &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;rbm spiral-rbm&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;chunk &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;find &apos;inputs &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;visible-chunks rbm&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:key&lt;/span&gt; #&apos;name&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;when chunk
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;clamp-striped-nodes samples chunk&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Define the DBN as a stack of two RBMs: one between the 3 inputs and
10 hidden features, the other between the 10 hidden features and the
encoding layer that&apos;s unsurprisingly has a single neuron:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defclass&lt;/span&gt;&lt;/i&gt; spiral-dbn &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dbn&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;:default-initargs&lt;/span&gt;
   &lt;span class=&quot;keyword&quot;&gt;:layers&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;list &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;list &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;constant-chunk &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;c0&lt;/span&gt;)&lt;/span&gt;
                       &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;gaussian-chunk &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;inputs &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; 3&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
                 &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;list &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;constant-chunk &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;c1&lt;/span&gt;)&lt;/span&gt;
                       &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;sigmoid-chunk &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;f1 &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; 10&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
                 &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;list &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;constant-chunk &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;c2&lt;/span&gt;)&lt;/span&gt;
                       &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;gaussian-chunk &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;f2 &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; 1&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;keyword&quot;&gt;:rbm-class&lt;/span&gt; &apos;spiral-rbm&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that by default, each pair of visible and hidden chunks is
connected by a &lt;code&gt;FULL-CLOUD&lt;/code&gt;, the simplest kind of connection.
&lt;code&gt;INPUTS&lt;/code&gt; via the cloud between &lt;code&gt;INPUTS&lt;/code&gt; and &lt;code&gt;F1&lt;/code&gt; contributes to the
activation of &lt;code&gt;F1&lt;/code&gt;: in the upward pass the values found in &lt;code&gt;INPUTS&lt;/code&gt;
are simply multiplied by a matrix of weights and the result is added
to the activation of &lt;code&gt;F1&lt;/code&gt;. Downward pass is similar.&lt;/p&gt;

&lt;p&gt;Once the activations are calculated according to what the clouds
prescribe, chunks take over control. Each chunk consists of a number
of nodes and defines a probability distribution over them based on
the activations. For instance, &lt;code&gt;SIGMOID-CHUNK&lt;/code&gt; is a binary chunk:
each node can take the value of 0 or 1 and the probability of 1 is
&lt;code&gt;1 / (1 + e^(-x))&lt;/code&gt; where &lt;code&gt;X&lt;/code&gt; is the activation of the node.&lt;/p&gt;

&lt;p&gt;Nodes in a &lt;code&gt;GAUSSIAN-CHUNK&lt;/code&gt; are normally distributed with means equal
to their activations and unit variance. In &lt;code&gt;SPIRAL-DBN&lt;/code&gt; above the
&lt;code&gt;INPUTS&lt;/code&gt; and the final code, &lt;code&gt;F2&lt;/code&gt;, are gaussian.&lt;/p&gt;

&lt;p&gt;Let&apos;s check out how it looks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let*&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dbn &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;spiral-dbn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
       &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dgraph &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;cl-dot:generate-graph-from-roots dbn &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;chunks dbn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;cl-dot:dot-graph dgraph &lt;span class=&quot;string&quot;&gt;&quot;spiral-dbn.png&quot;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:format&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:png&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/spiral-dbn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In a box the first line shows the class of the chunk and the number
of nodes in parens (omitted if 1), while the second line is the name
of the chunk itself. The constant chunks – in case you wonder –
provide the connected chunks with a bias. So far so good. Let&apos;s
train it RBM by RBM:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defclass&lt;/span&gt;&lt;/i&gt; spiral-rbm-trainer &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;rbm-cd-trainer&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; train-spiral-dbn &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&amp;amp;key &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;max-n-stripes 1&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dbn &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;spiral-dbn &lt;span class=&quot;keyword&quot;&gt;:max-n-stripes&lt;/span&gt; max-n-stripes&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dolist &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;rbm &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;rbms dbn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;train &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-sampler 50000&lt;/span&gt;)&lt;/span&gt;
             &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;spiral-rbm-trainer
                            &lt;span class=&quot;keyword&quot;&gt;:segmenter&lt;/span&gt;
                            &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;repeatedly &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;batch-gd-trainer
                                                       &lt;span class=&quot;keyword&quot;&gt;:momentum&lt;/span&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;flt 0.9&lt;/span&gt;)&lt;/span&gt;
                                                       &lt;span class=&quot;keyword&quot;&gt;:batch-size&lt;/span&gt; 100&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
             rbm&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    dbn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can unroll the DBN to a backprop network and add the sum of the
squared differences between the inputs and the reconstructions as the
error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defclass&lt;/span&gt;&lt;/i&gt; spiral-bpn &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defmethod&lt;/span&gt;&lt;/i&gt; mgl-train:set-input &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;samples &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn spiral-bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;clamp-striped-nodes samples &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;find-lump &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;chunk-lump-name &apos;inputs nil&lt;/span&gt;)&lt;/span&gt; bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; unroll-spiral-dbn &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dbn &amp;amp;key &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;max-n-stripes 1&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;multiple-value-bind &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defs&lt;/span&gt;&lt;/i&gt; inits&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;unroll-dbn dbn&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn-def `&lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;build-bpn &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;:class&lt;/span&gt; &apos;spiral-bpn
                                       &lt;span class=&quot;keyword&quot;&gt;:max-n-stripes&lt;/span&gt; ,max-n-stripes&lt;/span&gt;)&lt;/span&gt;
                      ,@&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defs&lt;/span&gt;&lt;/i&gt;
                      &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;sum-error &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-&amp;gt;sum-squared-error
                                  &lt;span class=&quot;keyword&quot;&gt;:x&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;lump &apos;,&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;chunk-lump-name &apos;inputs nil&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
                                  &lt;span class=&quot;keyword&quot;&gt;:y&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;lump &apos;,&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;chunk-lump-name
                                              &apos;inputs
                                              &lt;span class=&quot;keyword&quot;&gt;:reconstruction&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
                      &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;my-error &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;error-node &lt;span class=&quot;keyword&quot;&gt;:x&lt;/span&gt; sum-error&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn &lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;eval bpn-def&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
        &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;initialize-bpn-from-bm bpn dbn inits&lt;/span&gt;)&lt;/span&gt;
        bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The BPN looks a whole lot more complicated, but it does nothing more
than performing a full upward pass in the DBN and a full downward
pass:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let*&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dbn &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;spiral-dbn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
       &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;unroll-spiral-dbn dbn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
       &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dgraph &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;cl-dot:generate-graph-from-roots bpn &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;lumps bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;cl-dot:dot-graph dgraph &lt;span class=&quot;string&quot;&gt;&quot;spiral-bpn.png&quot;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:format&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:png&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/spiral-bpn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training it is as easy as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defclass&lt;/span&gt;&lt;/i&gt; spiral-bp-trainer &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bp-trainer&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
 
&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; train-spiral-bpn &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;train &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-sampler 50000&lt;/span&gt;)&lt;/span&gt;
         &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;spiral-bp-trainer
                        &lt;span class=&quot;keyword&quot;&gt;:segmenter&lt;/span&gt;
                        &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;repeatedly
                          &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;batch-gd-trainer
                                         &lt;span class=&quot;keyword&quot;&gt;:learning-rate&lt;/span&gt; &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;flt 0.01&lt;/span&gt;)&lt;/span&gt;
                                         &lt;span class=&quot;keyword&quot;&gt;:momentum&lt;/span&gt; &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;flt 0.9&lt;/span&gt;)&lt;/span&gt;
                                         &lt;span class=&quot;keyword&quot;&gt;:batch-size&lt;/span&gt; 100&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
         bpn&lt;/span&gt;)&lt;/span&gt;
  bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&apos;m tempted to dwell on pesky little details such as tracking errors,
but this entry is long enough already. Instead, load the &lt;code&gt;mgl-example&lt;/code&gt;
system and see what &lt;code&gt;example/spiral.lisp&lt;/code&gt; has in addition to what was
described. Evaluate the block commented forms at the end of the file
to see how training goes.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Tue, 29 Dec 2009 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Introduction to MGL (part 2)</title>
            <link>http://quotenil.com/introduction-to-mgl-part-2.html</link>
            <description>&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: This post out of date with regards to current MGL.
Please refer to the
&lt;a href=&quot;http://melisgl.github.io/mgl-pax-world/mgl-manual.html&quot; &gt;documentation&lt;/a&gt;
instead.&lt;/p&gt;

&lt;p&gt;After &lt;code&gt;@INTRODUCTION-TO-MGL-PART-1&lt;/code&gt;, today we are going to walk
through a small example and touch on the main concepts related to
learning within this library.
At the top of the food chain is the generic function &lt;code&gt;TRAIN&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defgeneric&lt;/span&gt;&lt;/i&gt; train &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;sampler trainer learner&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;:documentation&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;Train LEARNER with TRAINER on the examples from
SAMPLER. Before that TRAINER is initialized for LEARNER with
INITIALIZE-TRAINER. Training continues until SAMPLER is finished.&quot;&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A learner is anything that can be taught, which currently means it&apos;s
either a
&lt;a href=&quot;http://en.wikipedia.org/wiki/Backpropagation&quot; &gt;backpropagation&amp;nbsp;network&lt;/a&gt; (&lt;code&gt;BPN&lt;/code&gt;)
or some kind of boltzmann machine (&lt;code&gt;BM&lt;/code&gt;). The method with which a
learner is trained is decoupled from the learner itself and lives in
the trainer object. This makes it cleaner to support multiple
learning methods for the same learner: for instance, either gradient
descent (&lt;code&gt;BP-TRAINER&lt;/code&gt;) or conjugate gradients (&lt;code&gt;CG-BP-TRAINER&lt;/code&gt;) can
be used to train a BPN, and either contrastive
divergence (&lt;code&gt;RBM-CD-TRAINER&lt;/code&gt;) or persistent contrastive
divergence (&lt;code&gt;BM-PCD-TRAINER&lt;/code&gt;) can be used to train a restricted
boltzmann machine (&lt;code&gt;RBM&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The function &lt;code&gt;TRAIN&lt;/code&gt; takes training examples from
&lt;code&gt;SAMPLER&lt;/code&gt; (observing the batch size of the trainer, if applicable)
and calls &lt;code&gt;TRAIN-BATCH&lt;/code&gt; with the list of examples, the trainer and
the learner. This may be as simple as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defmethod&lt;/span&gt;&lt;/i&gt; train &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;sampler &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;trainer bp-trainer&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;while &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;not &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;finishedp sampler&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;train-batch &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;sample-batch sampler &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;n-inputs-until-update trainer&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
                 trainer bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ultimately, &lt;code&gt;TRAIN-BATCH&lt;/code&gt; arranges for the training examples to be
given as input to the learner (&amp;quot;clamped&amp;quot; on the input nodes of some
network) by &lt;code&gt;SET-INPUT&lt;/code&gt;; exactly how this should be done must be
customized. Then, in the case of &lt;code&gt;BP-TRAINER&lt;/code&gt;, the gradients are
calculated and added to the gradient accumulators that live in the
trainer. When the whole batch is processed the weights of the
network are updated according to the gradients.&lt;/p&gt;

&lt;p&gt;Let&apos;s put together a toy example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;use-package &lt;span class=&quot;keyword&quot;&gt;:mgl-util&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;use-package &lt;span class=&quot;keyword&quot;&gt;:mgl-train&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;use-package &lt;span class=&quot;keyword&quot;&gt;:mgl-gd&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;use-package &lt;span class=&quot;keyword&quot;&gt;:mgl-bp&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defclass&lt;/span&gt;&lt;/i&gt; linear-bpn &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defparameter&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;special&quot;&gt;*matrix*&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;matlisp:make-real-matrix &apos;&lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;1d0 2d0&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;3d0 4d0&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;5d0 6d0&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defparameter&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;special&quot;&gt;*bpn*&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;n-inputs 3&lt;/span&gt;)&lt;/span&gt;
        &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;n-outputs 2&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;build-bpn &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;:class&lt;/span&gt; &apos;linear-bpn&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;input &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;input-lump &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; n-inputs&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;weights &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;weight-lump &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;* n-inputs n-outputs&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;product &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;activation-lump &lt;span class=&quot;keyword&quot;&gt;:weights&lt;/span&gt; weights &lt;span class=&quot;keyword&quot;&gt;:x&lt;/span&gt; input&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;target &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;input-lump &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; n-outputs&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;sse &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;-&amp;gt;sum-squared-error &lt;span class=&quot;keyword&quot;&gt;:x&lt;/span&gt; target &lt;span class=&quot;keyword&quot;&gt;:y&lt;/span&gt; product&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
      &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;my-error &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;error-node &lt;span class=&quot;keyword&quot;&gt;:x&lt;/span&gt; sse&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defmethod&lt;/span&gt;&lt;/i&gt; set-input &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;samples &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;bpn linear-bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let*&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;input-nodes &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;nodes &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;find-lump &apos;input bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
         &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;target-nodes &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;nodes &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;find-lump &apos;target bpn&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
         &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;i-v &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;storage input-nodes&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;assert &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;= 1 &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;length samples&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;loop&lt;/span&gt;&lt;/i&gt; for i below &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;length i-v&lt;/span&gt;)&lt;/span&gt; do
          &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;setf &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;aref i-v i&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;elt &lt;span class=&quot;paren6&quot;&gt;(&lt;span class=&quot;code&quot;&gt;first samples&lt;/span&gt;)&lt;/span&gt; i&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
    &lt;span class=&quot;comment&quot;&gt;;; TARGET-NODES = INPUT-NODES * *MATRIX*
&lt;/span&gt;    &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;matlisp:gemm! 1d0 &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;reshape2 input-nodes 1 3&lt;/span&gt;)&lt;/span&gt; &lt;span class=&quot;special&quot;&gt;*matrix*&lt;/span&gt;
                   0d0 &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;reshape2 target-nodes 1 2&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;defun&lt;/span&gt;&lt;/i&gt; sample-input &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;loop&lt;/span&gt;&lt;/i&gt; repeat 3 collect &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;random 1d0&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;

&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;train &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;counting-function-sampler
                      &lt;span class=&quot;keyword&quot;&gt;:sampler&lt;/span&gt; #&apos;sample-input
                      &lt;span class=&quot;keyword&quot;&gt;:max-n-samples&lt;/span&gt; 10000&lt;/span&gt;)&lt;/span&gt;
       &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;bp-trainer
                      &lt;span class=&quot;keyword&quot;&gt;:segmenter&lt;/span&gt;
                      &lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;repeatedly
                        &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;batch-gd-trainer
                                       &lt;span class=&quot;keyword&quot;&gt;:learning-rate&lt;/span&gt; &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;flt 0.01&lt;/span&gt;)&lt;/span&gt;
                                       &lt;span class=&quot;keyword&quot;&gt;:momentum&lt;/span&gt; &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;flt 0.9&lt;/span&gt;)&lt;/span&gt;
                                       &lt;span class=&quot;keyword&quot;&gt;:batch-size&lt;/span&gt; 10&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
       &lt;span class=&quot;special&quot;&gt;*bpn*&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We subclassed &lt;code&gt;BPN&lt;/code&gt; as &lt;code&gt;LINEAR-BPN&lt;/code&gt; and hanged a &lt;code&gt;SET-INPUT&lt;/code&gt; method
on it. The &lt;code&gt;SAMPLES&lt;/code&gt; argument will be a sequence of samples returned
by the sampler passed to &lt;code&gt;TRAIN&lt;/code&gt;, that is, what &lt;code&gt;SAMPLE-INPUT&lt;/code&gt;
returns.&lt;/p&gt;

&lt;p&gt;The network multiplies &lt;code&gt;INPUT&lt;/code&gt; taken as a 1x3 matrix by &lt;code&gt;WEIGHTS&lt;/code&gt;
(initialized randomly), and the training aims to minimize the
squared error as calculated by the lump named &lt;code&gt;SSE&lt;/code&gt;. Note that
&lt;code&gt;SET-INPUT&lt;/code&gt; clamps both the real input and the target.&lt;/p&gt;

&lt;p&gt;We instantiate &lt;code&gt;BP-TRAINER&lt;/code&gt; that inherits from
&lt;code&gt;SEGMENTED-GD-TRAINER&lt;/code&gt;. Now, &lt;code&gt;SEGMENTED-GD-TRAINER&lt;/code&gt; itself does
precious little: it only delegates training to child trainers where
each child is supposed to be a &lt;code&gt;GD-TRAINER&lt;/code&gt; (with all the usual
knobs such as learning rate, momentum, weight decay, batch size,
etc). The mapping from segments (bpn lumps here) of the learner to
gd trainers is provided by the function in the &lt;code&gt;:SEGMENTER&lt;/code&gt;
argument. By using &lt;code&gt;REPEATEDLY&lt;/code&gt;, for now, we simply create a
distinct child trainer for each weight lump as it makes a function
that on each call evaluates the form in its body (as opposed to
&lt;a href=&quot;http://www.lispworks.com/documentation/HyperSpec/Body/f_cons_1.htm&quot; title=&quot;CONSTANTLY FUNCTION&quot;&gt;&lt;code&gt;CONSTANTLY&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;That&apos;s it without any bells and whistles. If all goes well,
&lt;code&gt;WEIGHTS&lt;/code&gt; should be trained to be equal to &lt;code&gt;*MATRIX*&lt;/code&gt;.
Inspect &lt;code&gt;(NODES (FIND-LUMP &apos;WEIGHTS *BPN*))&lt;/code&gt; to verify.&lt;/p&gt;

&lt;p&gt;Impatience satisfied, examine the &lt;code&gt;BUILD-BPN&lt;/code&gt; form in detail. The
&lt;code&gt;:CLASS&lt;/code&gt; argument is obvious, and the rest of the forms are a
sequence of bindings like in a &lt;a href=&quot;http://www.lispworks.com/documentation/HyperSpec/Body/s_let_l.htm&quot; title=&quot;LET* MGL-PAX:MACRO&quot;&gt;&lt;code&gt;LET*&lt;/code&gt;&lt;/a&gt;. The extra touches are that
the name of the variable to which a lump is bound is going to be
supplied as the &lt;code&gt;:NAME&lt;/code&gt; of the lump and an extra &lt;a href=&quot;http://www.lispworks.com/documentation/HyperSpec/Body/f_mk_ins.htm&quot; title=&quot;MAKE-INSTANCE FUNCTION&quot;&gt;&lt;code&gt;MAKE-INSTANCE&lt;/code&gt;&lt;/a&gt; is
added so&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;input &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;input-lump &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; n-inputs&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is something like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;make-instance &apos;input-lump &lt;span class=&quot;keyword&quot;&gt;:name&lt;/span&gt; &apos;input &lt;span class=&quot;keyword&quot;&gt;:size&lt;/span&gt; n-inputs&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One can replicate this with &lt;code&gt;MAKE-INSTANCE&lt;/code&gt; and &lt;code&gt;ADD-LUMP&lt;/code&gt;, but it&apos;s
more work. For ease of comprehension, the network can be visualized
by loading the &lt;code&gt;MGL-VISUALS&lt;/code&gt; system and:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren1&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;i&gt;&lt;span class=&quot;symbol&quot;&gt;let&lt;/span&gt;&lt;/i&gt; &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;&lt;span class=&quot;paren3&quot;&gt;(&lt;span class=&quot;code&quot;&gt;dgraph &lt;span class=&quot;paren4&quot;&gt;(&lt;span class=&quot;code&quot;&gt;cl-dot:generate-graph-from-roots &lt;span class=&quot;special&quot;&gt;*bpn*&lt;/span&gt; &lt;span class=&quot;paren5&quot;&gt;(&lt;span class=&quot;code&quot;&gt;lumps &lt;span class=&quot;special&quot;&gt;*bpn*&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;
  &lt;span class=&quot;paren2&quot;&gt;(&lt;span class=&quot;code&quot;&gt;cl-dot:dot-graph dgraph &lt;span class=&quot;string&quot;&gt;&quot;linear-bpn.png&quot;&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:format&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;:png&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;blog-files/linear-bpn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That&apos;s it for today, thank you for your kind attention.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Thu, 17 Dec 2009 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Introduction to MGL (part 1)</title>
            <link>http://quotenil.com/introduction-to-mgl-part-1.html</link>
            <description>&lt;p&gt;This is going to be the start of an introduction series on the
&lt;a href=&quot;http://cliki.net/MGL&quot; &gt;MGL&lt;/a&gt; Common Lisp machine learning library.
MGL focuses mainly on &lt;a href=&quot;http://en.wikipedia.org/wiki/Boltzmann_machine&quot; &gt;Boltzmann
Machines&lt;/a&gt; (BMs). In
fact, the few seemingly unrelated things it currently
offers (gradient descent, conjugate gradient, backprop) are directly
needed to implement the learning and fine tuning methods for
different kinds of BMs. But before venturing too far into specifics,
here is a quick glimpse at the bigger picture and the motivations.
Most of the current learning algorithms are based on shallow
architectures: they are fundamentally incapable of basing higher
level concepts on other, learned concepts. The most prominent
example of succesful shallow learners is Support Vector Machines,
for which there is a simple &lt;a href=&quot;http://cliki.net/cl-libsvm&quot; &gt;CL wrapper&lt;/a&gt;
around &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot; &gt;libsvm&lt;/a&gt;, but
that&apos;s a story for another day.&lt;/p&gt;

&lt;p&gt;On the other hand, deep learners are theorethically capable of
building abstraction on top of abstraction, the main hurdle in front
of their acceptance being that they don&apos;t exist or – more precisely
– we don&apos;t know how to train them.&lt;/p&gt;

&lt;p&gt;A good example of a deep learner is the multi-layer perceptron: with
only three layers it is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Universal_approximation_theorem&quot; &gt;universal
approximator&lt;/a&gt;
which is not a particularly difficult achievement, and the practical
implications of this result are not earth shattering: the number of
required training examples and hidden units can be very high and
generalization can be bad.&lt;/p&gt;

&lt;p&gt;Deep architectures mimic the layered organization of the brain and,
in theory, have better abstraction, generalization capability,
higher encoding effeciency. Of course, these qualities are strongly
related. While this has been known/suspected for a long time, it was
only recently that training of deep architectures &lt;a href=&quot;http://www.cs.toronto.edu/~hinton/science.pdf&quot; &gt;started to become
feasible&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of deep learners, Boltzmann machines deserve special attention as
they have demonstrated very good performance on a number of problems
and have a biologically plausible, local,
&lt;a href=&quot;http://en.wikipedia.org/wiki/Hebbian_theory&quot; &gt;Hebbian&lt;/a&gt; learning
rule.&lt;/p&gt;

&lt;p&gt;Now that you are sufficiently motivated, stick around and in
&lt;code&gt;@INTRODUCTION-TO-MGL-PART-2&lt;/code&gt;, we are going to see real examples.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Wed, 02 Dec 2009 00:00:00 +0000</pubDate>
        </item>
        <item>
            <title>Active Learning for cl-libsvm</title>
            <link>http://quotenil.com/active-learning-for-cl-libsvm.html</link>
            <description>&lt;p&gt;Along the lines of &lt;a href=&quot;http://mlbiomedicine.blogspot.com/2009/03/python-libsvm-or-on-hacking-libsvm.html&quot; &gt;active learning with python &amp;amp;
libsvm&lt;/a&gt;,
I &lt;a href=&quot;http://github.com/melisgl/cl-libsvm&quot; &gt;added&lt;/a&gt; support for
calculating distance of a point from the separating hyperplane to
&lt;a href=&quot;http://cliki.net/cl-libsvm&quot; &gt;cl-libsvm&lt;/a&gt;. In binary classification,
there is only one SVM involved and one hyperplane. However, with
N-class problems, there is a binary SVM for each of the $N(N-1)/2$
pairs of classes, and there are as many separating hyperplanes,
something the linked python code fails to take into account. As per
the &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f4151&quot; &gt;libsvm
FAQ&lt;/a&gt;, the
absolute value of the decision value (see &lt;code&gt;PREDICT-VALUES&lt;/code&gt;, wrapper
of &lt;code&gt;svm_predict_values&lt;/code&gt;) divided by the norm of the normal vector of
the separating hyperplane is the distance. &lt;code&gt;PREDICT-VALUES&lt;/code&gt; and
&lt;code&gt;MODEL-W2S&lt;/code&gt; are sufficient to calculate it. Note that among the
distributed binaries only the linux-x86 version has been recompiled
with the necessary changes, but patched sources are also included
for your recompiling pleasure.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Mon, 22 Jun 2009 01:00:00 +0100</pubDate>
        </item>
        <item>
            <title>2008 Computer Games Olympiad</title>
            <link>http://quotenil.com/2008-computer-games-olympiad.html</link>
            <description>&lt;p&gt;It seems that the competition has not been standing still (as opposed
to &lt;a href=&quot;hex/six/index.html&quot; &gt;Six&lt;/a&gt;), and this year marks the end of the
golden era. Congratulations to both Wolve and MoHex, who beat Six!
Thanks to Ryan Hayward, who again, kindly registered Six for the
&lt;a href=&quot;https://web.archive.org/web/20090121203454/http://www.grappa.univ-lille3.fr/icga/tournament.php?id=185&quot; &gt;Olympiad&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;About the future, I don&apos;t really plan on resuming work on
&lt;a href=&quot;http://en.wikipedia.org/wiki/Hex_(board_game)&quot; &gt;Hex&lt;/a&gt; in general (and
Six in particular) although losing does irk me a bit.&lt;/p&gt;
</description>
            <author>G&#xE1;bor Melis &lt;mega@retes.hu&gt;</author>
            <pubDate>Thu, 11 Dec 2008 00:00:00 +0000</pubDate>
        </item>
    </channel>
</rss>