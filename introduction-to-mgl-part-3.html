<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Introduction to MGL (part 3)
</title>
<link type='text/css' href='style.css' rel='stylesheet'>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width">
<script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
   <!-- Google tag (gtag.js) -->
<script async src='https://www.googletagmanager.com/gtag/js?id=G-7X64Q1D73F'></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7X64Q1D73F');
</script>
<script src='grid.js'></script>
<link rel='shortcut icon' type='image/png' href='favicon.png'>
<link rel='preconnect' href='https://fonts.googleapis.com'>
<link rel='preconnect' href='https://fonts.gstatic.com' crossorigin>
<link href='https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap'); </style>
<link rel='alternate' href='http://quotenil.com/blog.rss' type='application/rss+xml'/>

</head>
<body>
<div id="content-container">
<div id=toc>
 <div id=home>
  <a href=blog.html><code>(QUOTE NIL)</code></a>
 </div>
 <div id=links>
  Ramblings on <a href=ai.html>ai</a>, <a href=lisp.html>lisp</a>, <a href=tech.html>tech</a> 
  and <a href=personal.html>personal</a> topics by <a href=about-me.html>me</a>.
 </div>
</div><div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40INTRODUCTION-TO-MGL-PART-3-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@INTRODUCTION-TO-MGL-PART-3%20MGL-PAX:SECTION"></a></p>
<h1><a href="introduction-to-mgl-part-3.html">Introduction to MGL (part 3)</a></h1>
<p><span class='post-data'><em>Tags:</em> <a href="ai.html" title="ai"><code>ai</code></a>, <a href="lisp.html" title="lisp"><code>lisp</code></a>,&nbsp; <em>Date:</em> 2009-12-29</span></p>
<div class='br'></div>
<p><strong>UPDATE</strong> – This post is out of date with regards to current MGL.
Please refer to the
<a href="http://melisgl.github.io/mgl-pax-world/mgl-manual.html" >documentation</a>
instead.</p>
<p>In <a href="introduction-to-mgl-part-2.html" title="Introduction to MGL (part 2)">Introduction to MGL (part 2)</a>, we went through a trivial example of
a backprop network. I said before that the main focus is on
Boltzmann Machines so let's kill the suspense here and now by
cutting straight to the heart of the matter.</p>
<p><a href="http://cseweb.ucsd.edu/users/gary/pubs/cottrell-science-2006.pdf" >Cottrell's Science
article</a>
provides a clear and easy to follow description of the spiral
problem that we are going to implement. The executive summary is
that we want to train an auto-encoder: a network that reproduces its
input as output with a small encoding layer somewhere in between. By
forcing the information through the bottleneck of the encoding layer
the network should pick up a low dimensional code that represents
the input, thus performing dimensionality reduction.</p>
<p>The function under consideration is <code>f(x)</code>[x, sin(x), cos(x)]<code>. It
is suprisingly difficult to learn the mapping from</code>x<code>to</code>f(x)`. A
network architecture that is able to represent this transformation
has 3 inputs, 10 neurons in the next layer, 1 neuron in the encoding
layer, 10 neurons again in the reconstruction part and 3 in the
output layer. However, randomly initialized backpropagation fails at
learning this; a better solution is to first learn a Deep Belief
Network, \&quot;unroll\&quot; it to a backprop network and use backprop to
fine tune the weights.</p>
<p>A <a href="http://www.scholarpedia.org/article/Boltzmann_machine#Learning_deep_networks_by_composing_restricted_Boltzmann_machines" >Deep Belief
Network</a>
is just a stack of <a href="http://www.scholarpedia.org/article/Boltzmann_machine#Restricted_Boltzmann_machines" >Restricted Boltzmann
Machines</a>.
An RBM is a BM restricted to be a two layer network with no
intralayer connections. The lower layer is called the visible, and
the higher layer is called hidden layer because from the point of
view of a single RBM, it is the visible layer that's connected to –
maybe indirectly – to external stimuli. In the upward pass of a DBN,
where the low level representations are subsequently transformed
into higher level ones by the constituent RBMs, the values of the
hidden units are clamped onto the visible units of the next RBM. In
other words, an RBM shares its visible and hidden layers with the
hidden and visible layers of the RBM below and above, respectively,
respectively.</p>
<p>Let's start with a few utility functions:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> sample-spiral <span class="paren2">(<span class="code"></span>)</span>
  <span class="paren2">(<span class="code">random <span class="paren3">(<span class="code">flt <span class="paren4">(<span class="code">* 4 pi</span>)</span></span>)</span></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> make-sampler <span class="paren2">(<span class="code">n</span>)</span>
  <span class="paren2">(<span class="code">make-instance 'counting-function-sampler
                 <span class="keyword">:max-n-samples</span> n
                 <span class="keyword">:sampler</span> #'sample-spiral</span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> clamp-array <span class="paren2">(<span class="code">x array start</span>)</span>
  <span class="paren2">(<span class="code">setf <span class="paren3">(<span class="code">aref array <span class="paren4">(<span class="code">+ start 0</span>)</span></span>)</span> x
        <span class="paren3">(<span class="code">aref array <span class="paren4">(<span class="code">+ start 1</span>)</span></span>)</span> <span class="paren3">(<span class="code">sin x</span>)</span>
        <span class="paren3">(<span class="code">aref array <span class="paren4">(<span class="code">+ start 2</span>)</span></span>)</span> <span class="paren3">(<span class="code">cos x</span>)</span></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> clamp-striped-nodes <span class="paren2">(<span class="code">samples striped</span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren3">(<span class="code"><span class="paren4">(<span class="code">nodes <span class="paren5">(<span class="code">storage <span class="paren6">(<span class="code">nodes striped</span>)</span></span>)</span></span>)</span></span>)</span>
    <span class="paren3">(<span class="code"><i><span class="symbol">loop</span></i> for sample in samples
          for stripe upfrom 0
          do <span class="paren4">(<span class="code"><i><span class="symbol">with-stripes</span></i> <span class="paren5">(<span class="code"><span class="paren6">(<span class="code">stripe striped start</span>)</span></span>)</span>
               <span class="paren5">(<span class="code">clamp-array sample nodes start</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>
<p>Subclass <code>RBM</code> and define <code>SET-INPUT</code> using the above utilites:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defclass</span></i> spiral-rbm <span class="paren2">(<span class="code">rbm</span>)</span> <span class="paren2">(<span class="code"></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defmethod</span></i> mgl-train:set-input <span class="paren2">(<span class="code">samples <span class="paren3">(<span class="code">rbm spiral-rbm</span>)</span></span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren3">(<span class="code"><span class="paren4">(<span class="code">chunk <span class="paren5">(<span class="code">find 'inputs <span class="paren6">(<span class="code">visible-chunks rbm</span>)</span> <span class="keyword">:key</span> #'name</span>)</span></span>)</span></span>)</span>
    <span class="paren3">(<span class="code">when chunk
      <span class="paren4">(<span class="code">clamp-striped-nodes samples chunk</span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>
<p>Define the DBN as a stack of two RBMs: one between the 3 inputs and
10 hidden features, the other between the 10 hidden features and the
encoding layer that's unsurprisingly has a single neuron:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defclass</span></i> spiral-dbn <span class="paren2">(<span class="code">dbn</span>)</span>
  <span class="paren2">(<span class="code"></span>)</span>
  <span class="paren2">(<span class="code"><span class="keyword">:default-initargs</span>
   <span class="keyword">:layers</span> <span class="paren3">(<span class="code">list <span class="paren4">(<span class="code">list <span class="paren5">(<span class="code">make-instance 'constant-chunk <span class="keyword">:name</span> 'c0</span>)</span>
                       <span class="paren5">(<span class="code">make-instance 'gaussian-chunk <span class="keyword">:name</span> 'inputs <span class="keyword">:size</span> 3</span>)</span></span>)</span>
                 <span class="paren4">(<span class="code">list <span class="paren5">(<span class="code">make-instance 'constant-chunk <span class="keyword">:name</span> 'c1</span>)</span>
                       <span class="paren5">(<span class="code">make-instance 'sigmoid-chunk <span class="keyword">:name</span> 'f1 <span class="keyword">:size</span> 10</span>)</span></span>)</span>
                 <span class="paren4">(<span class="code">list <span class="paren5">(<span class="code">make-instance 'constant-chunk <span class="keyword">:name</span> 'c2</span>)</span>
                       <span class="paren5">(<span class="code">make-instance 'gaussian-chunk <span class="keyword">:name</span> 'f2 <span class="keyword">:size</span> 1</span>)</span></span>)</span></span>)</span>
    <span class="keyword">:rbm-class</span> 'spiral-rbm</span>)</span></span>)</span></span></code></pre>
<p>Note that by default, each pair of visible and hidden chunks is
connected by a <code>FULL-CLOUD</code>, the simplest kind of connection.
<code>INPUTS</code> via the cloud between <code>INPUTS</code> and <code>F1</code> contributes to the
activation of <code>F1</code>: in the upward pass the values found in <code>INPUTS</code>
are simply multiplied by a matrix of weights and the result is added
to the activation of <code>F1</code>. Downward pass is similar.</p>
<p>Once the activations are calculated according to what the clouds
prescribe, chunks take over control. Each chunk consists of a number
of nodes and defines a probability distribution over them based on
the activations. For instance, <code>SIGMOID-CHUNK</code> is a binary chunk:
each node can take the value of 0 or 1 and the probability of 1 is
<code>1 / (1 + e^(-x))</code> where <code>X</code> is the activation of the node.</p>
<p>Nodes in a <code>GAUSSIAN-CHUNK</code> are normally distributed with means equal
to their activations and unit variance. In <code>SPIRAL-DBN</code> above the
<code>INPUTS</code> and the final code, <code>F2</code>, are gaussian.</p>
<p>Let's check out how it looks:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">let*</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">dbn <span class="paren4">(<span class="code">make-instance 'spiral-dbn</span>)</span></span>)</span>
       <span class="paren3">(<span class="code">dgraph <span class="paren4">(<span class="code">cl-dot:generate-graph-from-roots dbn <span class="paren5">(<span class="code">chunks dbn</span>)</span></span>)</span></span>)</span></span>)</span>
  <span class="paren2">(<span class="code">cl-dot:dot-graph dgraph <span class="string">"spiral-dbn.png"</span> <span class="keyword">:format</span> <span class="keyword">:png</span></span>)</span></span>)</span></span></code></pre>
<p><img src="blog-files/spiral-dbn.png" alt="" /></p>
<p>In a box the first line shows the class of the chunk and the number
of nodes in parens (omitted if 1), while the second line is the name
of the chunk itself. The constant chunks – in case you wonder –
provide the connected chunks with a bias. So far so good. Let's
train it RBM by RBM:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defclass</span></i> spiral-rbm-trainer <span class="paren2">(<span class="code">rbm-cd-trainer</span>)</span> <span class="paren2">(<span class="code"></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> train-spiral-dbn <span class="paren2">(<span class="code">&amp;key <span class="paren3">(<span class="code">max-n-stripes 1</span>)</span></span>)</span>
  <span class="paren2">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren3">(<span class="code"><span class="paren4">(<span class="code">dbn <span class="paren5">(<span class="code">make-instance 'spiral-dbn <span class="keyword">:max-n-stripes</span> max-n-stripes</span>)</span></span>)</span></span>)</span>
    <span class="paren3">(<span class="code">dolist <span class="paren4">(<span class="code">rbm <span class="paren5">(<span class="code">rbms dbn</span>)</span></span>)</span>
      <span class="paren4">(<span class="code">train <span class="paren5">(<span class="code">make-sampler 50000</span>)</span>
             <span class="paren5">(<span class="code">make-instance 'spiral-rbm-trainer
                            <span class="keyword">:segmenter</span>
                            <span class="paren6">(<span class="code">repeatedly <span class="paren1">(<span class="code">make-instance 'batch-gd-trainer
                                                       <span class="keyword">:momentum</span> <span class="paren2">(<span class="code">flt 0.9</span>)</span>
                                                       <span class="keyword">:batch-size</span> 100</span>)</span></span>)</span></span>)</span>
             rbm</span>)</span></span>)</span>
    dbn</span>)</span></span>)</span></span></code></pre>
<p>Now we can unroll the DBN to a backprop network and add the sum of the
squared differences between the inputs and the reconstructions as the
error:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defclass</span></i> spiral-bpn <span class="paren2">(<span class="code">bpn</span>)</span> <span class="paren2">(<span class="code"></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defmethod</span></i> mgl-train:set-input <span class="paren2">(<span class="code">samples <span class="paren3">(<span class="code">bpn spiral-bpn</span>)</span></span>)</span>
  <span class="paren2">(<span class="code">clamp-striped-nodes samples <span class="paren3">(<span class="code">find-lump <span class="paren4">(<span class="code">chunk-lump-name 'inputs nil</span>)</span> bpn</span>)</span></span>)</span></span>)</span>

<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> unroll-spiral-dbn <span class="paren2">(<span class="code">dbn &amp;key <span class="paren3">(<span class="code">max-n-stripes 1</span>)</span></span>)</span>
  <span class="paren2">(<span class="code">multiple-value-bind <span class="paren3">(<span class="code"><i><span class="symbol">defs</span></i> inits</span>)</span> <span class="paren3">(<span class="code">unroll-dbn dbn</span>)</span>
    <span class="paren3">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren4">(<span class="code"><span class="paren5">(<span class="code">bpn-def `<span class="paren6">(<span class="code">build-bpn <span class="paren1">(<span class="code"><span class="keyword">:class</span> 'spiral-bpn
                                       <span class="keyword">:max-n-stripes</span> ,max-n-stripes</span>)</span>
                      ,@<i><span class="symbol">defs</span></i>
                      <span class="paren1">(<span class="code">sum-error <span class="paren2">(<span class="code">-&gt;sum-squared-error
                                  <span class="keyword">:x</span> <span class="paren3">(<span class="code">lump ',<span class="paren4">(<span class="code">chunk-lump-name 'inputs nil</span>)</span></span>)</span>
                                  <span class="keyword">:y</span> <span class="paren3">(<span class="code">lump ',<span class="paren4">(<span class="code">chunk-lump-name
                                              'inputs
                                              <span class="keyword">:reconstruction</span></span>)</span></span>)</span></span>)</span></span>)</span>
                      <span class="paren1">(<span class="code">my-error <span class="paren2">(<span class="code">error-node <span class="keyword">:x</span> sum-error</span>)</span></span>)</span></span>)</span></span>)</span></span>)</span>
      <span class="paren4">(<span class="code"><i><span class="symbol">let</span></i> <span class="paren5">(<span class="code"><span class="paren6">(<span class="code">bpn <span class="paren1">(<span class="code">eval bpn-def</span>)</span></span>)</span></span>)</span>
        <span class="paren5">(<span class="code">initialize-bpn-from-bm bpn dbn inits</span>)</span>
        bpn</span>)</span></span>)</span></span>)</span></span>)</span></span></code></pre>
<p>The BPN looks a whole lot more complicated, but it does nothing more
than performing a full upward pass in the DBN and a full downward
pass:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">let*</span></i> <span class="paren2">(<span class="code"><span class="paren3">(<span class="code">dbn <span class="paren4">(<span class="code">make-instance 'spiral-dbn</span>)</span></span>)</span>
       <span class="paren3">(<span class="code">bpn <span class="paren4">(<span class="code">unroll-spiral-dbn dbn</span>)</span></span>)</span>
       <span class="paren3">(<span class="code">dgraph <span class="paren4">(<span class="code">cl-dot:generate-graph-from-roots bpn <span class="paren5">(<span class="code">lumps bpn</span>)</span></span>)</span></span>)</span></span>)</span>
  <span class="paren2">(<span class="code">cl-dot:dot-graph dgraph <span class="string">"spiral-bpn.png"</span> <span class="keyword">:format</span> <span class="keyword">:png</span></span>)</span></span>)</span></span></code></pre>
<p><img src="blog-files/spiral-bpn.png" alt="" /></p>
<p>Training it is as easy as:</p>
<pre><code><span class="code"><span class="paren1">(<span class="code"><i><span class="symbol">defclass</span></i> spiral-bp-trainer <span class="paren2">(<span class="code">bp-trainer</span>)</span> <span class="paren2">(<span class="code"></span>)</span></span>)</span>
 
<span class="paren1">(<span class="code"><i><span class="symbol">defun</span></i> train-spiral-bpn <span class="paren2">(<span class="code">bpn</span>)</span>
  <span class="paren2">(<span class="code">train <span class="paren3">(<span class="code">make-sampler 50000</span>)</span>
         <span class="paren3">(<span class="code">make-instance 'spiral-bp-trainer
                        <span class="keyword">:segmenter</span>
                        <span class="paren4">(<span class="code">repeatedly
                          <span class="paren5">(<span class="code">make-instance 'batch-gd-trainer
                                         <span class="keyword">:learning-rate</span> <span class="paren6">(<span class="code">flt 0.01</span>)</span>
                                         <span class="keyword">:momentum</span> <span class="paren6">(<span class="code">flt 0.9</span>)</span>
                                         <span class="keyword">:batch-size</span> 100</span>)</span></span>)</span></span>)</span>
         bpn</span>)</span>
  bpn</span>)</span></span></code></pre>
<p>I'm tempted to dwell on pesky little details such as tracking errors,
but this entry is long enough already. Instead, load the <code>mgl-example</code>
system and see what <code>example/spiral.lisp</code> has in addition to what was
described. Evaluate the block commented forms at the end of the file
to see how training goes.</p>
<img src="blog-files/die.png" alt="end-of-post" />
  </div>
</div>
<script>$('#page-toc').toc({'selectors': ''});</script>
</body>
</html>
