<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>On the Design of Matrix Libraries</title>
<link type='text/css' href='style.css' rel='stylesheet'>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width">
   <script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
   </script>
   <!-- Google tag (gtag.js) -->
<script async src='https://www.googletagmanager.com/gtag/js?id=G-7X64Q1D73F'></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7X64Q1D73F');
</script>
<link rel='shortcut icon' type='image/png' href='favicon.png'>
<link rel='preconnect' href='https://fonts.googleapis.com'>
<link rel='preconnect' href='https://fonts.gstatic.com' crossorigin>
<link href='https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap'); </style>

</head>
<body>
<div id="content-container">
<div id=toc>
 <div id=home>
  <a href=blog.html>(QUOTE NIL)</a>
 </div>
 <div id=links>
  Ramblings on <a href=ai.html>ai</a>, <a href=lisp.html>lisp</a>, <a
  href=tech.html>tech</a> and <a href=personal.html>personal</a> topics by <a
  href=about-me.html>me</a>.
 </div>
</div><div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40ON-THE-DESIGN-OF-MATRIX-LIBRARIES-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@ON-THE-DESIGN-OF-MATRIX-LIBRARIES%20MGL-PAX:SECTION"></a></p>

<h1><a href="on-the-design-of-matrix-libraries.html">On the Design of Matrix Libraries</a></h1>

<p><em>Tags</em>: <a href="ai.html" title="ai"><code>ai</code></a>, <a href="lisp.html" title="lisp"><code>lisp</code></a>, <em>Date</em>: <code>2015-02-26</code></p>

<p><strong>UPDATE</strong>: <em>2020-05-03</em> â€“ Things have been moving fast. This is a
non-issue in Tensorflow and possibly in other frameworks, as well.</p>

<p>I believe there is one design decision in
<a href="http://melisgl.github.io/mgl-pax-world/mat-manual.html" >MGL-MAT</a>
that has far reaching consequences: to make a single matrix object
capable of storing multiple representations of the same data and let
operations decide which representation to use based on what's the
most convenient or efficient, without having to even know about all
the possible representations.</p>

<p>This allows existing code to keep functioning if support for
diagonal matrices (represented as a 1d array) lands and one can pick
and choose the operations performance critical enough to implement
with diagonals.</p>

<p>Adding support for matrices that, for instance, live on a remote
machine is thus possible with a new facet type (MAT lingo for
representation) and existing code would continue to work (albeit
possibly slowly). Then one could optimize the bottleneck operations
by sending commands over the network instead of copying data.</p>

<p>Contrast this with what I understand to be the status quo over on
the Python side. The specialized Python array libs (cudamat,
gpuarray, cudandarray) try to be drop-in replacements for - or at
least similar to - numpy.ndarray with various degrees of success.
There is lots of explicit conversion going on between ndarray and
these CUDA blobs and adding new representations would make this
exponentionally worse.</p>

<p><a href="http://torch.ch/" >Torch</a> (Lua) also has CUDA and non-CUDA tensors
are separate types, and copying between main and GPU memory is
explicit which leads to pretty much the same problems.</p>

<p>All of this is kind of understandable. When one thinks in terms of
single-dispatch (i.e. <code>object.method()</code>), this kind of design will
often emerge. With muliple-dispatch, data representation and
operations are more loosely coupled. The facet/operation duality of
MGL-MAT is reminiscent of how CLOS classes and generic functions
relate to each other. The anology is best if objects are allowed to
shapeshift to fit the method signatures.</p>

<p>Speaking of multiple-dispatch, by making the operations generic
functions following some kind of protocol to decide which facets and
implementation to use would decouple facets further. Ultimately,
this could make the entire CUDA related part of MGL-MAT an add-on.</p>
  </div>
</div>
<script>$('#page-toc').toc({'selectors': ''});</script>
</body>
</html>
