<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Introduction to MGL (part 1)</title>
<link type='text/css' href='style.css' rel='stylesheet'>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width">
   <script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
   </script>
   <!-- Google tag (gtag.js) -->
<script async src='https://www.googletagmanager.com/gtag/js?id=G-7X64Q1D73F'></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7X64Q1D73F');
</script>
<link rel='shortcut icon' type='image/png' href='favicon.png'>
<link rel='preconnect' href='https://fonts.googleapis.com'>
<link rel='preconnect' href='https://fonts.gstatic.com' crossorigin>
<link href='https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap'); </style>

</head>
<body>
<div id="content-container">
<div id=toc>
 <div id=home>
  <a href=blog.html>(QUOTE NIL)</a>
 </div>
 <div id=links>
  Ramblings on <a href=ai.html>ai</a>, <a href=lisp.html>lisp</a>, <a
  href=tech.html>tech</a> and <a href=personal.html>personal</a> topics by <a
  href=about-me.html>me</a>.
 </div>
</div><div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40INTRODUCTION-TO-MGL-PART-1-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@INTRODUCTION-TO-MGL-PART-1%20MGL-PAX:SECTION"></a></p>

<h1><a href="introduction-to-mgl-part-1.html">Introduction to MGL (part 1)</a></h1>

<p><em>Tags</em>: <a href="ai.html" title="ai"><code>ai</code></a>, <a href="lisp.html" title="lisp"><code>lisp</code></a>, <em>Date</em>: <code>2009-12-02</code></p>

<p><strong>UPDATE</strong>: This post out of date with regards to current MGL.
Please refer to the
<a href="http://melisgl.github.io/mgl-pax-world/mgl-manual.html" >documentation</a>
instead.</p>

<p>This is going to be the start of an introduction series on the
<a href="http://cliki.net/MGL" >MGL</a> Common Lisp machine learning library.
MGL focuses mainly on <a href="http://en.wikipedia.org/wiki/Boltzmann_machine" >Boltzmann
Machines</a> (BMs). In
fact, the few seemingly unrelated things it currently
offers (gradient descent, conjugate gradient, backprop) are directly
needed to implement the learning and fine tuning methods for
different kinds of BMs. But before venturing too far into specifics,
here is a quick glimpse at the bigger picture and the motivations.</p>

<p>Most of the current learning algorithms are based on shallow
architectures: they are fundamentally incapable of basing higher
level concepts on other, learned concepts. The most prominent
example of succesful shallow learners is Support Vector Machines,
for which there is a simple <a href="http://cliki.net/cl-libsvm" >CL wrapper</a>
around <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" >libsvm</a>, but
that's a story for another day.</p>

<p>On the other hand, deep learners are theorethically capable of
building abstraction on top of abstraction, the main hurdle in front
of their acceptance being that they don't exist or - more precisely
- we don't know how to train them.</p>

<p>A good example of a deep learner is the multi-layer perceptron: with
only three layers it is a <a href="http://en.wikipedia.org/wiki/Universal_approximation_theorem" >universal
approximator</a>
which is not a particularly difficult achievement, and the practical
implications of this result are not earth shattering: the number of
required training examples and hidden units can be very high and
generalization can be bad.</p>

<p>Deep architectures mimic the layered organization of the brain and,
in theory, have better abstraction, generalization capability,
higher encoding effeciency. Of course, these qualities are strongly
related. While this has been known/suspected for a long time, it was
only recently that training of deep architectures <a href="http://www.cs.toronto.edu/~hinton/science.pdf" >started to become
feasible</a>.</p>

<p>Of deep learners, boltzmann machines deserve special attention as
they have demonstrated very good performance on a number of problems
and have a biologically plausible, local,
<a href="http://en.wikipedia.org/wiki/Hebbian_theory" >Hebbian</a> learning
rule.</p>

<p>Now that you are sufficiently motivated, stick around and in
<a href="introduction-to-mgl-part-2.html" title="Introduction to MGL (part 2)">Introduction to MGL (part 2)</a>, we are going to see real examples.</p>
  </div>
</div>
<script>$('#page-toc').toc({'selectors': ''});</script>
</body>
</html>
