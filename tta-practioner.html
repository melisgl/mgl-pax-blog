<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Practioner's Guide to Two-Tailed Averaging</title>
<link type='text/css' href='style.css' rel='stylesheet'>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width">
   <script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
   </script>
   <link rel='preconnect' href='https://fonts.googleapis.com'>
<link rel='preconnect' href='https://fonts.gstatic.com' crossorigin>
<link href='https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap'); </style>

</head>
<body>
<div id="content-container">
<div id=toc>
 <div id=home>
  <a href=blog.html>(QUOTE NIL)</a>
 </div>
 <div id=links>
  Ramblings on <a href=ai.html>ai</a>, <a href=lisp.html>lisp</a>, <a
  href=tech.html>tech</a> and <a href=personal.html>personal</a> topics by <a
  href=about-me.html>me</a>.
 </div>
</div><div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40TTA-PRACTIONER-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@TTA-PRACTIONER%20MGL-PAX:SECTION"></a></p>

<h1><a href="tta-practioner.html">Practioner's Guide to Two-Tailed Averaging</a></h1>

<p><em>Tags</em>: <a href="ai.html" title="ai"><code>ai</code></a>, <em>Date</em>: <code>2022-12-06</code></p>

<p>This is a complement to the <a href="https://arxiv.org/abs/2209.12581" >Two-Tailed Averaging
paper</a>, approached from the
direction of what I think is a fairly common technique: averaging
checkpoints.</p>

<p>We want to speed up training and improve generalization. One way to
do that is by averaging weights from optimization, and that's big
win (e.g. <a href="https://arxiv.org/abs/1708.02182" >1</a>, <a href="https://arxiv.org/abs/1803.05407" >2</a>, <a href="https://arxiv.org/abs/2209.14981" >3</a>). For example, while
training a language model for the down-stream task of summarization,
we can save checkpoints periodically and average the model weights
from the last 10 or so checkpoints to produce the final solution.
This is pretty much what <a href="https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/" >Stochastic Weight
Averaging</a> (SWA).</p>

<h3>Problems with SWA</h3>

<p>There is a number of problems with SWA:</p>

<ul>
<li><p>10, the averaging length, must be chosen to maximize performance
  on summarization.</p></li>
<li><p>A naive way to find the averaging length is to do a single
  training run and then search backwards extending the average one
  checkpoint at a time, which needs lots of storage and computation.
  Another option, doing multiple training runs each told to start
  averaging at a predefined point pays a steep price in computation
  for lower storage costs.</p></li>
<li><p>To control the costs, we can lower checkpointing fequency, but
  does that make results worse? We can test that with multiple
  training runs, and pay the cost there.</p></li>
<li><p>Also, how do we know when to stop training? We ideally want to
  stop training the language model when summarization works best
  with the optimal averaging length at that point. That means the
  naive search has to be run periodically making it even more
  expensive.</p></li>
</ul>

<p>In summary, working with SWA is tricky because:</p>

<ul>
<li><p>The averaging length is a hyperparameter that's costly to set (it
  is coupled to other hyperparameters especially to the length of
  training and the learning rate).</p></li>
<li><p>Determining the averaging length after training is both costly (in
  storage and/or computation) and suboptimal (can miss early
  solutions).</p></li>
</ul>

<h3>Two-Tailed Averaging</h3>

<p>These are the issues Two-Tailed Averaging tackles. The algorithm
needs storage for only two sets of weights (constant storage cost)
and performance (e.g. summarization) to be evaluated periodically.
In return, it provides a weight average of approximately optimal
length at all optimization steps. Now we can start training that
language model, periodically evaluating how the averaged weights are
doing at summarization. We can stop the training run anytime if it's
getting worse.</p>

<p>This is how Two-Tailed Averaged (orange) compares to SWA (green)
tuned to start averaging at the point that's optimal for final
validation loss:</p>

<p><img src="blog-files/tta-vs-swa.png" alt="TTA (orange) vs SWA (green)" /></p>

<h3>Downsampling weights</h3>

<p>In its proposed form, Two-Tailed Averaging incorporates every set of
weights produced by the optimizer in both averages it maintains.
This is good because <a href="https://jmlr.org/papers/v18/16-595.html" >Tail</a>, also known as
<a href="https://arxiv.org/abs/1109.5647" >Suffix</a>, averaging theory has nice things to say
about convergence to a local optimum in this setting. However, in a
memory constrained situation, these averages will not fit on the
GPU/TPU, so we must move the weights off the device to add them to
the averages (which may be in RAM or on disk). Moving stuff off the
device can be slow, so we might want to do that, say, every 20
optimization steps. Obviously, downsampling the weights too much
will affect the convergence rate, so there is a tradeoff.</p>

<h3>Learning rate</h3>

<p>Note that in our experiments with Two-Tailed Averaging, we used a
constant learning rate motivated by the fact that the closely
related method of Tail Averaging guarantees optimal convergence rate
learning rate in such a setting. The algorithm should work with
decreasing learning rates, but would require modification for
cyclical schedules.</p>

<h3>Related works</h3>

<ul>
<li><p><a href="https://arxiv.org/abs/1803.05407" >SWA</a> averages the last $K$ checkpoints.</p></li>
<li><p><a href="https://arxiv.org/abs/2209.14981" >LAWA</a> averages the $K$ most recent checkpoints, so it
  produces reasonable averages from early on (unlike SWA), but $K$
  still needs to be set manually.</p></li>
<li><p><a href="https://arxiv.org/abs/1708.02182" >NT-ASGD</a> starts averaging when the validation loss does
  not improve for a fixed number of optimization steps, which trades
  one hyperparameter for another, and it is sensitive to noise in
  the raw validation loss. online.</p></li>
</ul>

<p><strong>Adaptivity</strong>: SWA and LAWA have hyperparameters that directly
control the averaging length; NT-ASGD still has one, but its effect
is more indirect. <strong>Anytime</strong>: LAWA provides an average at all
times, SWA and NT-ASGD don't. <strong>Optimality</strong>: The final averages of
SWA and LAWA are optimal if their hyperparameters are well-tuned;
intermediate results of LAWA are unlikely to be optimal; NT-ASGD can
miss the right time to start averaging.</p>

<h3>Summary</h3>

<p>Two-Tailed Averaging can be thought of as online SWA with no
hyperparameters. It is a great option when training runs take a long
(or even an a priori unknown amount of) time, and when we could do
without optimizing yet another hyperparameter.</p>
  </div>
</div>
<script>$('#page-toc').toc({'selectors': ''});</script>
</body>
</html>
