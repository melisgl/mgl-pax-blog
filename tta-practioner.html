<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Two-tailed Averaging: The ML Practioner's Guide</title>
<link type='text/css' href='style.css' rel='stylesheet'/>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script type="text/javascript" async
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
   </script>
   <link href="https://fonts.googleapis.com/css2?family=Literata:ital,opsz,wght@0,7..72,400;0,7..72,700;1,7..72,400;1,7..72,700&display=swap" rel="stylesheet">
<style> @import url('https://fonts.googleapis.com/css2?family=Literata:ital,opsz,wght@0,7..72,400;0,7..72,700;1,7..72,400;1,7..72,700&display=swap'); </style>

</head>
<body>
<div id="content-container">
<div id="toc">
<div class="menu-block"><span class="menu-block-title">me</span><ul><li><a href="http://quotenil.com">blog</a></li><li><a href="mailto:mega@retes.hu">mega@retes.hu</a></li><li><a href="mega.gpg.asc">gpg key</a></li><li><a href="http://github.com/melisgl/">github/melisgl</a></li><li><a href="https://mastodon.social/@melisgl">mastodon.social/@melisgl</a></li><li><a href="https://twitter.com/GaborMelis">twitter/GaborMelis</a></li><li><a href="http://discord.com/users/melisgl#0879">discord/melisgl#0879</a></li><li><a href="https://www.linkedin.com/in/melisgabor/">linkedin/melisgabor</a></li></ul></div><div class="menu-block"><span class="menu-block-title">categories</span><ul><li><a href="http://quotenil.com/blog.html">all</a></li><li><a href="http://quotenil.com/category-personal.html">personal</a></li><li><a href="http://quotenil.com/category-tech.html">tech</a></li><li><a href="http://quotenil.com/category-lisp.html">lisp</a></li><li><a href="http://quotenil.com/category-ai.html">ai</a></li></ul></div><div id="page-toc">
</div>
<div id="toc-footer"><ul><li><a href="https://github.com/melisgl/mgl-pax">[generated by MGL-PAX]</a></li></ul></div>
</div>
<div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40TTA-PRACTIONER-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@TTA-PRACTIONER%20MGL-PAX:SECTION"></a></p>

<p><span class="outer-navigation"><span class="navigation"> <a href="#MGL-PAX-BLOG:@TTA-PRACTIONER%20MGL-PAX:SECTION" title="Two-tailed Averaging: The ML Practioner's Guide">&#8634;</a></span></span></p>

<h1><a href="#MGL-PAX-BLOG:@TTA-PRACTIONER%20MGL-PAX:SECTION">Two-tailed Averaging: The ML Practioner's Guide</a></h1>

<p><em>Tags</em>: <a href="blog.html#MGL-PAX-BLOG:@BLOG%20MGL-PAX:SECTION" title="Blog">Blog</a>, <a href="ai.html#MGL-PAX-BLOG:@AI%20MGL-PAX:SECTION" title="AI">AI</a>, <em>Date</em>: 2022-12-02</p>

<p>This is a complement to the Two-tailed Averaging
<a href="https://arxiv.org/abs/2209.12581" >paper</a>.</p>

<p>We want to speed up training and improve generalization. One way to
do that is by averaging weights from optimization. For example,
while training a language model for the down-stream task of
summarization, we can save checkpoints periodically, and average the
weights in last 10 checkpoints to produce the final solution. This
is pretty much what <a href="https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/" >Stochastic Weight Averaging</a> (SWA)
does.</p>

<h4>Problems with SWA</h4>

<p>There is a number of problems with SWA:</p>

<ul>
<li><p>10, the &quot;averaging length&quot;, must be chosen to maximize performance
  on summarization.</p></li>
<li><p>Naive search for the averaging length needs lots of storage and
  computation. For example, saving a lot of checkpoints from a
  single optimization and performing a search after training has
  very high storage cost. Another option, doing multiple training
  runs each told to start averaging at a predefined point pays a
  steep price in computation for lower storage costs.</p></li>
<li><p>To control the costs, we can lower checkpointing fequency, but
  does that make results worse? We can test that with multiple
  training runs, and pay the cost there.</p></li>
<li><p>Also, how do we know when to stop training? We ideally want to
  stop training the language model when summarization works best
  with the optimal averaging length at that point. That means the
  naive search has to be run periodically making it even more
  expensive.</p></li>
</ul>

<p>In summary, working with SWA is tricky because:</p>

<ul>
<li><p>The averaging length is a costly to set hyperparameter (that is
  coupled to other hyperparameters especially to the length of
  training and the learning rate).</p></li>
<li><p>Determining the averaging length after training is both costly (in
  storage and/or computation) and suboptimal (can miss early
  solutions).</p></li>
</ul>

<h4>Two-Tailed Averaging</h4>

<p>These are the issues Two-tailed Averaging tackles. The algorithm
needs storage for two sets of weights (constant storage cost) and
performance (e.g. summarization) to be evaluated periodically. In
return, it provides a weight average of approximately optimal length
at all optimization steps. Now we can start training that language
model, periodically evaluating how the averaged weights are doing at
summarization. We can stop the training run anytime if it's getting
worse.</p>

<p>This is how Two-tailed Averaged (orange) compares to SWA (green)
tuned to start averaging at the point that's optimal for final
validation loss:</p>

<p><img src="blog-files/tta-vs-swa.png" alt="TTA (orange) vs SWA (green)" /></p>

<h4>Downsampling weights</h4>

<p>In its proposed form, Two-tailed Averaging incorporates every set of
weights produced by the optimizer in both averages it maintains.
This is good because <a href="https://jmlr.org/papers/v18/16-595.html" >Tail</a>, also known as
<a href="https://arxiv.org/abs/1109.5647" >Suffix</a>, averaging theory has nice things to say
about convergence to a local optimum in this setting. However, in a
memory constrained situation, these averages will not fit on the
GPU/TPU, so we must move the weights off the device to add them to
the average (which may be in RAM or on disk). Moving stuff off the
device can be slow, so we might want to do that, say, every 20
optimization steps. Obviously, downsampling the weights too much
will affect the convergence rate, so there is a tradeoff.</p>

<h4>Learning rate</h4>

<p>Note that with Two-tailed averaging, we use constant learning rate
motivated by the fact that the closely related method of Tail
averaging guarantees optimal convergence rate learning rate in such
a setting.</p>
  </div>
</div>
<script>$('#page-toc').toc({'selectors': 'h1'});</script>
</body>
</html>
