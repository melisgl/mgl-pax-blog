<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Practitioner's Guide to Two-Tailed Averaging
</title>
<link type='text/css' href='style.css' rel='stylesheet'>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width">
   <script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
   </script>
   <!-- Google tag (gtag.js) -->
<script async src='https://www.googletagmanager.com/gtag/js?id=G-7X64Q1D73F'></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7X64Q1D73F');
</script>
<script>
function adjustHeight() {
  document.querySelectorAll('pre, img').forEach(el => {
    if (el.alt == 'end-of-post' || el.alt == 'about-me-die') {
      return;
    }
    // This includes padding, which the current CSS doesn't have;
    // offsetHeight doesn't but is rounded.
    let currentHeight = el.getBoundingClientRect().height;
    let lineHeight = parseFloat(window.getComputedStyle(document.body).lineHeight);
    let targetHeight = Math.ceil(currentHeight / lineHeight) * lineHeight;
    if (el.parentNode.className != 'padding-wrapper') {
      w = document.createElement('div');
      w.className = 'padding-wrapper';
      el.replaceWith(w);
      w.appendChild(el);
    }
    let extraPadding = targetHeight - currentHeight;
    if (extraPadding < 0.75*lineHeight) {
      extraPadding += lineHeight;
    }
    let remPx = parseFloat(getComputedStyle(document.documentElement).fontSize)
    extraPadding = extraPadding / 2 /remPx;
    el.parentNode.style.paddingTop = `${extraPadding}rem`;
    el.parentNode.style.paddingBottom = `${extraPadding}rem`;
  });
}
window.addEventListener('load', adjustHeight);
window.addEventListener('resize', adjustHeight);
</script>
<link rel='shortcut icon' type='image/png' href='favicon.png'>
<link rel='preconnect' href='https://fonts.googleapis.com'>
<link rel='preconnect' href='https://fonts.gstatic.com' crossorigin>
<link href='https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap'); </style>
<link rel='alternate' href='http://quotenil.com/blog.rss' type='application/rss+xml'/>

</head>
<body>
<div id="content-container">
<div id=toc>
 <div id=home>
  <a href=blog.html>(QUOTE NIL)</a>
 </div>
 <div id=links>
  Ramblings on <a href=ai.html>ai</a>, <a href=lisp.html>lisp</a>, <a
  href=tech.html>tech</a> and <a href=personal.html>personal</a> topics by <a
  href=about-me.html>me</a>.
 </div>
</div><div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40TTA-PRACTITIONER-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@TTA-PRACTITIONER%20MGL-PAX:SECTION"></a></p>
<h1><a href="tta-practitioner.html">Practitioner's Guide to Two-Tailed Averaging</a></h1>
<p><span class='post-data'><em>Tags:</em> <a href="ai.html" title="ai"><code>ai</code></a>,&nbsp; <em>Date:</em> 2022-12-06</span></p>
<div class='br'></div>
<p>This is a complement to the <a href="https://arxiv.org/abs/2209.12581" >Two-Tailed Averaging
paper</a>, approached from the
direction of what I think is a fairly common technique: averaging
checkpoints.</p>
<p>We want to speed up training and improve generalization. One way to
do that is by averaging weights from optimization, and that's a big
win (e.g. <a href="https://arxiv.org/abs/1708.02182" title="nt-asgd">1</a>, <a href="https://arxiv.org/abs/1803.05407" title="SWA">2</a>, <a href="https://arxiv.org/abs/2209.14981" title="LAWA">3</a>). For example, while
training a language model for the down-stream task of summarization,
we can save checkpoints periodically and average the model weights
from the last 10 or so checkpoints to produce the final solution.
This is pretty much what <a href="https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/" >Stochastic Weight
Averaging</a> (SWA) does.</p>
<h2>Problems with SWA</h2>
<p>There is a number of problems with SWA:</p>
<ul>
<li><p>The averaging length (e.g. 10) must be chosen to maximize
  performance on summarization.</p></li>
<li><p>A naive way to find the averaging length is to do a single
  training run and then search backwards extending the average one
  checkpoint at a time, which needs lots of storage and computation.
  Another option, doing multiple training runs each told to start
  averaging at a predefined point pays a steep price in computation
  for lower storage costs.</p></li>
<li><p>To control the costs, we can lower checkpointing fequency, but
  does that make results worse? We can test it with multiple
  training runs and pay the cost there.</p></li>
<li><p>Also, how do we know when to stop training? We ideally want to
  stop training the language model when summarization works best
  with the optimal averaging length at that point. That means the
  naive search has to be run periodically making it even more
  expensive.</p></li>
</ul>
<p>In summary, working with SWA is tricky because:</p>
<ul>
<li><p>The averaging length is a hyperparameter that's costly to set (it
  is coupled with other hyperparameters especially with the length
  of training and the learning rate).</p></li>
<li><p>Determining the averaging length after training is both costly (in
  storage and/or computation) and suboptimal (can miss early
  solutions).</p></li>
</ul>
<p>These are the issues Two-Tailed Averaging tackles.</p>
<h2>Two-Tailed Averaging</h2>
<p>The algorithm needs storage for only two sets of weights (constant
storage cost) and performance (e.g. of summarization) to be
evaluated periodically. In return, it provides a weight average of
approximately optimal length at all optimization steps. Now we can
start training that language model, periodically evaluating how the
averaged weights are doing at summarization. We can stop the
training run any time if it's getting worse.</p>
<p>This is how Two-Tailed Averaged (orange) compares to SWA (green)
tuned to start averaging at the point that's optimal for final
validation loss:</p>
<p><img src="blog-files/tta-vs-swa.png" alt="2TA (orange) vs SWA (green)" /></p>
<h2>The Algorithm</h2>
<p>The core algorithm maintains two weight averages. Both averages are
over the most recent weights weight produced by the optimizer, but
they differ in length (i.e. how many weights they average). As the
optimizer produces new sets of weights, they are added to both
averages. We periodically evaluate the performance of our model with
each average. If the short average (the one that currently has fewer
weights averaged) does at least as well as the long average
according to some arbitrary evaluation function, then we empty the
long average, which will now be the short one.</p>
<pre><code><span class="code"><span class="comment"># Initialize the short (s, sw) and long averages (l, lw). s and l are
</span><span class="comment"># the number of weights averaged (the "averaging lengths"). sw and lw
</span><span class="comment"># are the averaged weights.
</span>s, sw, l, lw = 0, 0, 0, 0

<span class="comment"># Update the averages with the latest weights from the optimizer.
</span><span class="special">def</span><span
class="keyword"> update_2ta</span><span class="paren1">(<span class="code">w</span>)</span>:
  <span class="symbol">global</span> s, sw, l, lw
  <span class="symbol">assert</span> s &lt;= l
  s, sw = s+1, <span class="paren1">(<span class="code">s*sw + w</span>)</span>/<span class="paren1">(<span class="code">s+1</span>)</span>
  l, lw = l+1, <span class="paren1">(<span class="code">l*lw + w</span>)</span>/<span class="paren1">(<span class="code">l+1</span>)</span>

<span class="comment"># Evaluate the model with the short-, the long-, and the
</span><span class="comment"># non-averaged weights. Based on the results, adapt the length of
</span><span class="comment"># the averages. Return three values: the best evaluation results,
</span><span class="comment"># the corresponding weights and averaging length.
</span><span class="special">def</span><span
class="keyword"> evaluate_2ta</span><span class="paren1">(<span class="code">w, evaluate</span>)</span>:
  <span class="symbol">global</span> s, sw, l, lw
  <span class="comment"># Evaluate the non-averaged weights w, the short and the long average.
</span>  f1, fs, fl = evaluate<span class="paren1">(<span class="code">w</span>)</span>, evaluate<span class="paren1">(<span class="code">sw</span>)</span>, evaluate<span class="paren1">(<span class="code">lw</span>)</span>
  is_first_eval = <span class="paren1">(<span class="code">s == l</span>)</span>
  <span class="comment"># If the short average is better, then *switch*: empty the long
</span>  <span class="comment"># average, which is now the shorter one.
</span>  <span class="symbol">if</span> fs &lt;= fl:
    s, l, lw, fl = 0, s, sw, fs
  <span class="symbol">if</span> f1 &lt;= fl:
    <span class="comment"># The non-averaged weights performed better. This may happen in
</span>    <span class="comment"># the very early stages of training.
</span>    <span class="symbol">if</span> is_first_eval:
      <span class="comment"># If there has never been a switch (s == l), then f1 is probably
</span>      <span class="comment"># still improving fast so reset both averages.
</span>      s, l = 0, 0
    <span class="symbol">return</span> f1, w, 1
  else:
    <span class="comment"># Return the long average.
</span>    <span class="symbol">return</span> fl, lw, l</span></code></pre>
<p>In addition to the core algorithm, the code above has some extra
logic to deal with the non-averaged weights being better than the
averaged ones.</p>
<p>Let's write a fake a training loop that optimizes $f(x)=x^2$.</p>
<pre><code><span class="code"><span class="symbol">import</span> random

<span class="special">def</span><span
class="keyword"> test_2ta_simple</span><span class="paren1">(<span class="code"></span>)</span>:
  <span class="special">def</span><span
class="keyword"> f</span><span class="paren1">(<span class="code">w</span>)</span>:
    <span class="symbol">return</span> w**2
  <span class="special">def</span><span
class="keyword"> df_dw</span><span class="paren1">(<span class="code">w</span>)</span>:
    <span class="comment"># Simulate stochasticity due to e.g. minibatching.
</span>    <span class="symbol">return</span> 2*w + random.uniform<span class="paren1">(<span class="code">-1.0, 1.0</span>)</span>
  lr = 0.5
  w = 3.14
  <span class="symbol">for</span> i <span class="symbol">in</span> range<span class="paren1">(<span class="code">1, 2001</span>)</span>:
    w = w - lr*df_dw<span class="paren1">(<span class="code">w</span>)</span>
    update_2ta<span class="paren1">(<span class="code">w</span>)</span>
    <span class="symbol">if</span> i % 100 == 0:
      f_2ta, w_2ta, l_2ta = evaluate_2ta<span class="paren1">(<span class="code">w, f</span>)</span>
      <span class="symbol">print</span><span class="paren1">(<span class="code">f<span class="string">'i={i:4d}: f(w_i)={f(w):7.3f},'</span>
            f<span class="string">' f(w_2ta)={f_2ta:7.3f}, l={l_2ta:4d}'</span></span>)</span></span></code></pre>
<p>We added some noise to the gradients in <code>df_dw</code> to make it more like
training a neural net with SGD. Anyway, we take 2000 optimization
steps, calling <code>update_2ta</code> on most but calling
<code>update_and_evaluate_2ta</code> every 100 steps. Running
<code>test_2ta_simple</code>, we get something like this:</p>
<pre><code><span class="code">i= 100: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.108, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 100
i= 200: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.011, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 200
i= 300: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.098, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 200
i= 400: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.085, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 300
i= 500: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.221, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 200
i= 600: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.185, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 300
i= 700: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.019, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 400
i= 800: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.180, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 500
i= 900: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.161, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 600
i=1000: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.183, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 700
i=1100: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.057, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 800
i=1200: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.045, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 900
i=1300: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.051, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l=1000
i=1400: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.010, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l= 900
i=1500: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.012, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l=1000
i=1600: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.168, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l=1100
i=1700: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.001, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l=1200
i=1800: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.020, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l=1300
i=1900: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.090, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l=1400
i=2000: f<span class="paren1">(<span class="code">w_i</span>)</span>=0.115, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=0.000, l=1500</span></code></pre>
<p>In the above, <code>f(w_i)</code> is the loss with the non-averaged weights,
<code>f(w_2ta)</code> is the loss with the weights provided by <span
class='lnum'>2TA</span>, and <code>l</code> is the number of weights averaged.
We see that with the high, constant learning rate, SGD keeps jumping
around the optimum, and while <span class='lnum'>2TA</span> does the
same, its jitter is way smaller (it's beyond the three significant
digits printed here). Also, the length of the average increases
almost monotonically but not quite due to the switching logic.</p>
<p>OK, that was easy. Let's now do something a bit more involved, where
the function being optimized changes. We will change the loss
function to $f(x) = (x-m)^2$ where $m$ is set randomly every 400
steps. We will deal with this non-stationarity by resetting the long
average if it has not improved for a while.</p>
<pre><code><span class="code"><span class="special">def</span><span
class="keyword"> reset_2ta_long_average</span><span class="paren1">(<span class="code"></span>)</span>:
  <span class="symbol">global</span> s, sw, l, lw
  s, sw, l, lw = 0, 0, s, sw

<span class="special">def</span><span
class="keyword"> test_2ta_non_stationary</span><span class="paren1">(<span class="code"></span>)</span>:
  optimum = 0
  <span class="special">def</span><span
class="keyword"> f</span><span class="paren1">(<span class="code">w</span>)</span>:
    <span class="symbol">return</span> <span class="paren1">(<span class="code">w-optimum</span>)</span>**2
  <span class="special">def</span><span
class="keyword"> df_dw</span><span class="paren1">(<span class="code">w</span>)</span>:
    <span class="comment"># Simulate stochasticity due to e.g. minibatching.
</span>    <span class="symbol">return</span> 2*w - 2*optimum + random.uniform<span class="paren1">(<span class="code">-1.0, 1.0</span>)</span>
  lr = 0.5
  w = 3.14
  best_f = float<span class="paren1">(<span class="code"><span class="string">"inf"</span></span>)</span>
  best_iteration = 0
  <span class="symbol">for</span> i <span class="symbol">in</span> range<span class="paren1">(<span class="code">1, 2001</span>)</span>:
    w = w - lr*df_dw<span class="paren1">(<span class="code">w</span>)</span>
    update_2ta<span class="paren1">(<span class="code">w</span>)</span>
    <span class="symbol">if</span> i % 400 == 0:
      optimum = random.uniform<span class="paren1">(<span class="code">-10.0, 10.0</span>)</span>
      <span class="symbol">print</span><span class="paren1">(<span class="code">f<span class="string">'setting optimum={optimum:.3f}'</span></span>)</span>
    <span class="symbol">if</span> i % 100 == 0:
      f_2ta, w_2ta, l_2ta = evaluate_2ta<span class="paren1">(<span class="code">w, f</span>)</span>
      <span class="symbol">print</span><span class="paren1">(<span class="code">f<span class="string">'i={i:4d}: f(w_i)={f(w):7.3f},'</span>
            f<span class="string">' f(w_2ta)={f_2ta:7.3f}, l={l_2ta:4d}'</span>,
            end=<span class="string">''</span></span>)</span>
      <span class="symbol">if</span> l_2ta &gt; 1 <span class="symbol">and</span> f_2ta &lt; best_f:
        best_f = f_2ta
        best_iteration = i
        <span class="symbol">print</span><span class="paren1">(<span class="code"></span>)</span>
      <span class="symbol">elif</span> best_iteration + 1 &lt;= i:
        <span class="comment"># Reset heuristic: the results of the long average have not
</span>        <span class="comment"># improved for a while, let's reset it so that it may adapt
</span>        <span class="comment"># quicker.
</span>        <span class="symbol">print</span><span class="paren1">(<span class="code"><span class="string">' Reset!'</span></span>)</span>
        reset_2ta_long_average<span class="paren1">(<span class="code"></span>)</span>
        best_f = float<span class="paren1">(<span class="code"><span class="string">"inf"</span></span>)</span>
        best_iteration = 0</span></code></pre>
<p>We can see that <span class='lnum'>2TA</span> adapts to the non-stationarity in a reasonable
way although the reset heuristic gets triggered spuriously a couple
of times:</p>
<pre><code><span class="code">i= 100: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.008, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.005, l= 100
i= 200: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.060, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 100
i= 300: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.004, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 100
setting optimum=9.691
i= 400: f<span class="paren1">(<span class="code">w_i</span>)</span>= 87.194, f<span class="paren1">(<span class="code">w_2ta</span>)</span>= 87.194, l=   1 Reset!
i= 500: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.002, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 100
i= 600: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.033, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 200 Reset!
i= 700: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.126, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 200
setting optimum=9.899
i= 800: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.022, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.022, l=   1 Reset!
i= 900: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.004, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.003, l= 100
i=1000: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.094, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 100
i=1100: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.146, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 100
setting optimum=3.601
i=1200: f<span class="paren1">(<span class="code">w_i</span>)</span>= 35.623, f<span class="paren1">(<span class="code">w_2ta</span>)</span>= 35.623, l=   1 Reset!
i=1300: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.113, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.001, l= 100
i=1400: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.166, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 200
i=1500: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.112, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 200
setting optimum=6.662
i=1600: f<span class="paren1">(<span class="code">w_i</span>)</span>= 11.692, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  9.409, l= 300 Reset!
i=1700: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.075, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 100
i=1800: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.229, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 200 Reset!
i=1900: f<span class="paren1">(<span class="code">w_i</span>)</span>=  0.217, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=  0.000, l= 100
setting optimum=-8.930
i=2000: f<span class="paren1">(<span class="code">w_i</span>)</span>=242.481, f<span class="paren1">(<span class="code">w_2ta</span>)</span>=242.481, l=   1 Reset!</span></code></pre>
<p>Note that in these examples, the evaluation function in <span
class='lnum'>2TA</span> was the training loss, but <span
class='lnum'>2TA</span> is intended for when the evaluation function
measures performance on the validation set or on a down-stream
task (e.g. summarization).</p>
<h2>Scaling to Large Models</h2>
<p>In its proposed form, Two-Tailed Averaging incorporates every set of
weights produced by the optimizer in both averages it maintains.
This is good because <a href="https://jmlr.org/papers/v18/16-595.html" title="Tail Averaging">Tail Averaging</a>, also known as
<a href="https://arxiv.org/abs/1109.5647" title="Suffix Averaging">Suffix Averaging</a>, theory has nice things to say about convergence
to a local optimum of the training loss in this setting. However, in
a memory constrained situation, these averages will not fit on the
GPU/TPU, so we must move the weights off the device to add them to
the averages (which may be in RAM or on disk). Moving stuff off the
device can be slow, so we might want to do that, say, every 20
optimization steps. Obviously, downsampling the weights too much
will affect the convergence rate, so there is a tradeoff.</p>
<h2>Learning Rate</h2>
<p>Note that in our experiments with Two-Tailed Averaging, we used a
constant learning rate motivated by the fact that the closely
related method of Tail Averaging guarantees optimal convergence rate
learning rate in such a setting. The algorithm should work with
decreasing learning rates but would require modification for
cyclical schedules.</p>
<h2>Related Works</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/1803.05407" title="SWA">SWA</a> averages the last $K$ checkpoints.</p></li>
<li><p><a href="https://arxiv.org/abs/2209.14981" title="LAWA">LAWA</a> averages the $K$ most recent checkpoints, so it
  produces reasonable averages from early on (unlike SWA), but $K$
  still needs to be set manually.</p></li>
<li><p><a href="https://arxiv.org/abs/1708.02182" title="nt-asgd">nt-asgd</a> starts averaging when the validation loss has not
  improved for a fixed number of optimization steps, which trades
  one hyperparameter for another, and it is sensitive to noise in
  the raw validation loss.</p></li>
</ul>
<p><strong>Adaptivity</strong>: SWA and LAWA have hyperparameters that directly
control the averaging length; NT-ASGD still has one, but its effect
is more indirect. <strong>Anytimeness</strong>: LAWA provides an average at all
times, SWA and NT-ASGD don't. <strong>Optimality</strong>: The final averages of
SWA and LAWA are optimal if their hyperparameters are well-tuned;
intermediate results of LAWA are unlikely to be optimal; NT-ASGD can
miss the right time to start averaging.</p>
<h2>Summary</h2>
<p>Two-Tailed Averaging can be thought of as online SWA with no
hyperparameters. It is a great option when training runs take a long
(or even an a priori unknown amount of) time, and when we could do
without optimizing yet another hyperparameter.
<div class='br'></div><p>Comment on
<a href="https://twitter.com/GaborMelis/status/1600479387937144833" >Twitter</a>
or <a href="https://mastodon.social/@melisgl/109472579530491223" >Mastodon</a>.</p></p>
<img src="blog-files/die.png" alt="end-of-post" />
  </div>
</div>
<script>$('#page-toc').toc({'selectors': ''});</script>
</body>
</html>
