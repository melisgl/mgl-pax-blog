<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>Deep Boltzmann Machine on MNIST</title>
<link type='text/css' href='style.css' rel='stylesheet'>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width">
   <script src="jquery.min.js"></script>
<script src="toc.min.js"></script>
<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [['$','$']],
         processEscapes: true
       }
     });
   </script>
   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
   </script>
   <!-- Google tag (gtag.js) -->
<script async src='https://www.googletagmanager.com/gtag/js?id=G-7X64Q1D73F'></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7X64Q1D73F');
</script>
<link rel='shortcut icon' type='image/png' href='favicon.png'>
<link rel='preconnect' href='https://fonts.googleapis.com'>
<link rel='preconnect' href='https://fonts.gstatic.com' crossorigin>
<link href='https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap' rel='stylesheet'>
<style> @import url('https://fonts.googleapis.com/css2?family=Gentium+Book+Plus:ital,wght@0,400;0,700;1,400;1,700&display=swap'); </style>

</head>
<body>
<div id="content-container">
<div id=toc>
 <div id=home>
  <a href=blog.html>(QUOTE NIL)</a>
 </div>
 <div id=links>
  Ramblings on <a href=ai.html>ai</a>, <a href=lisp.html>lisp</a>, <a
  href=tech.html>tech</a> and <a href=personal.html>personal</a> topics by <a
  href=about-me.html>me</a>.
 </div>
</div><div id="content">
<p><a id="x-28MGL-PAX-BLOG-3A-40DEEP-BOLTZMANN-MACHINE-ON-MNIST-20MGL-PAX-3ASECTION-29"></a>
<a id="MGL-PAX-BLOG:@DEEP-BOLTZMANN-MACHINE-ON-MNIST%20MGL-PAX:SECTION"></a></p>

<h1><a href="deep-boltzmann-machine-on-mnist.html">Deep Boltzmann Machine on MNIST</a></h1>

<p><em>Tags</em>: <a href="ai.html" title="ai"><code>ai</code></a>, <a href="lisp.html" title="lisp"><code>lisp</code></a>, <em>Date</em>: <code>2010-01-18</code></p>

<p>Let me interrupt the flow of the
<a href="http://cliki.net/MGL" >MGL</a> introduction series with a short report
on what I learnt playing with <a href="http://www.cs.toronto.edu/~hinton/absps/dbm.pdf" >Deep Boltzmann
Machines</a>. First,
lots of thanks to Ruslan Salakhutdinov, then at <a href="http://www.cs.toronto.edu/~rsalakhu/" >University of
Toronto</a> now at
<a href="http://web.mit.edu/~rsalakhu/www/" >MIT</a>, for making the Matlab
source <a href="http://web.mit.edu/~rsalakhu/www/DBM.html" >code</a> for the
<a href="http://yann.lecun.com/exdb/mnist/" >MNIST</a> digit classification
problem available.</p>

<p>The linked <a href="http://www.cs.toronto.edu/~hinton/absps/dbm.pdf" >paper</a>
claims a record of 99.05% in classification accuracy on the
permutation invariant task (no prior knowledge of geometry). A
previous approach trained a
<a href="http://www.scholarpedia.org/article/Deep_belief_networks" >DBN</a> in
an unsupervised manner and fine tuned it with backpropagation. Now
there is one more step: turning the DBN into a DBM (Deep Boltzmann
Machine) and tune it further before handing the baton over to
backprop. While in a DBN the constituent RBMs are trained one by
one, the DBM is trained as a whole which, in theory, allows it to
reconcile bottom-up and top-down signals, i.e. what you see and what
you think.</p>

<p><img src="blog-files/mnist-2-dbm.png" alt="" /></p>

<p>In the diagram above, as before, dark gray boxes are constants (to
provide the connected chunks with biases), inputs are colored mid
gray while hidden features are light gray. <code>INPUTS</code> is where the
28x28 pixel image is clamped and <code>LABEL</code> is a softmax chunk for the
10 digit classes.</p>

<p>In the Matlab code, there are a number of prominent features that
may or may not be important to this result:</p>

<ul>
<li><p>The second RBM gets the the correct label as input which
  conveniently allows tracking classification accuracy during its
  training, but also - more importantly - forces the top-level
  features to be somewhat geared towards reconstruction of labels
  and thus classification.</p></li>
<li><p>A sparsity term is added to the gradient. Sparse representations
  are often better for classification.</p></li>
</ul>

<p>Focusing only on what makes DBM learning tick, I tried a few
variants of the basic approach. All of them start with the same DBN
whose RBMs are trained for 100 epochs each:</p>

<p><img src="blog-files/mnist-2-dbn-training.png" alt="" /></p>

<p>DBN training finishes with around 97.77%, averaging 97.9% in the
last 10 epochs.</p>

<p>On to the DBM. As the baseline, the DBM was not trained at all and
the BPN did not get the marginals of the approximate posterior as
inputs as prescribed in the paper, only the normal input. It's as if
the DBN were unrolled into a BPN directly. Surprisingly, this
baseline is already at 99.00% at the end of BPN training (all
reported accuracies are averages from the last 10 epochs of
training).</p>

<p>The second variant performs DBM training but without any sparsity
term and gets 99.07%. The third is using a sparsity penalty (&quot;normal
sparsity&quot; in the diagram) for units in opposing opposing layers on
at the same time and nets 99.08%. The fourth is just a translation
of the sparsity penalty from the Matlab code. This one is
named &quot;cheating sparsity&quot; because it - perhaps in an effort to
reduce variance of the gradient - changes weights according to the
average activation levels of units connected by them. Anyway, this
last one reaches 99.09%.</p>

<p><img src="blog-files/mnist-2-dbm-training.png" alt="" /></p>

<p><img src="blog-files/mnist-2-bpn-training.png" alt="" /></p>

<p>To reduce <a href="http://en.wikipedia.org/wiki/Publication_bias" >publication
bias</a> a bit, let me
mention some experiments that were found to have no effect:</p>

<ul>
<li><p>In an effort to see whether DBM training is held back by high
  variance of the gradient estimates a batch size of 1000 (instead
  of 100) was tested for a hundred epochs after the usual 500. There
  was no improvement.</p></li>
<li><p>In the BPN label weights and biases were initialized from the DBM.
  This initial advantage diminishes gradually and by the end of
  training there is nothing (+0.01%) between the initialized and
  uninitialized variants. Nevertheless, all results and diagrams are
  from runs with label weights initialized.</p></li>
<li><p>The matlab code goes out of its way to compute negative phase
  statistics from the <em>expectations</em> of the units in <code>F1</code> and <code>F2</code>
  supposedly to help with variance of estimates and this turned out
  to be very important: with the same calculation based on the
  sampled values DBM classification deteriorated. Using the
  expectations for chunks <code>INPUTS</code> and <code>LABEL</code> did not help, though.</p></li>
</ul>

<p>What I take home from these experiments is that from the
considerable edge of DBM over DBN training only a small fraction
remains by the end of BPN training and that the additional sparsity
constraint accounts for very little in this setup.</p>
  </div>
</div>
<script>$('#page-toc').toc({'selectors': ''});</script>
</body>
</html>
